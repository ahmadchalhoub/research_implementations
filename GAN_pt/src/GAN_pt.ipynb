{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the implementation of a very simple GAN (Generative Adversarial Model).\n",
    "\n",
    "This is first trial at implementing an algorithm from a research paper; in this case, 'Generative Adversarial Nets' (https://arxiv.org/pdf/1406.2661.pdf).\n",
    "\n",
    "For this implementation, I am using many resources and basically just dipping my toes in the world of applied ML, as I hope to to gain two main outcomes of this. First, I would like to improve my actual coding skills when it comes to implementing ML/DL algorithms, as I would like to eventually be able to actually read complex publications and implement them myself. This will also help me stay updated with the new advancements happening in this field. Second, it will hopefully keep me in touch with the actual technical part and prevent me from simply being an 'implementer of github projects'. \n",
    "\n",
    "PS: the actual code for this implementation is a combination of the code shown in both the following Medium article: https://towardsdatascience.com/converting-deep-learning-research-papers-to-code-f-f38bbd87352f as well as the following YouTube tutorial: https://www.youtube.com/watch?v=OljTVUVzPpM. I just made very small changes and added more detailed comments for more clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "from torch import optim\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize device, hyperparameters, and create latent space (noise).\n",
    "Load the MNIST dataset and specify necessary tansforms and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the GPU if cuda is available. otherwise use the CPU\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "lr = 3e-4               # learning rate = 3e-4\n",
    "image_dim = 28*28*1     # 28*28*1=784 (MNIST data)\n",
    "batch_size = 32         # batch size\n",
    "num_epochs = 50        # number of epochs to train GAN\n",
    "\n",
    "torch.manual_seed(7)\n",
    "noise_dim = 64\n",
    "#noise = torch.randn((batch_size, noise_dim))\n",
    "\n",
    "# transforms.Compose allows us to combine multiple changes that we wish to do to an input image;\n",
    "# in this case, the image is first converted to a Pytorch tensor and then normalized\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "mnist_images = torchvision.datasets.MNIST(\n",
    "    root = '/home/achalhoub/dev/research_implementations/GAN_pt/input/',\n",
    "    transform=transform)\n",
    "\n",
    "#x, _ = mnist_images[7777]\n",
    "#plt.imshow(x.numpy()[0], cmap='gray')\n",
    "loader = torch.utils.data.DataLoader(mnist_images, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Discriminator, which looks at an input image and outputs a '0' of it thinks the image is fake and a '1' if it thinks the image is real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a custom nn module, used for custom models.\n",
    "# a sequential model, which is made up of the input layer of\n",
    "# size 32*32*3 and a ReLU activation function and a single hidden layer\n",
    "# with 10000 nodes, and finally an output layer of size 10000 with a sigmoid \n",
    "# activation function, is instantiated.\n",
    "\n",
    "# a method for the forward pass is also created, which first flattens the\n",
    "# input image and then performs a forward pass of the image through the Discriminator.\n",
    "# this method then returns the output result that is coming from the sigmoid function.\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(image_dim, 10000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10000, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        # flatten the input image and perform a forward pass\n",
    "        out = self.linear(img)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Generator, which creates an output image tensor (fake image) using input random noise. The size of the fake image is the same size as that of the real image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a custom nn module, used for custom models.\n",
    "# a sequential model, which is made up of the input layer of\n",
    "# size 100 and a LeakyReLU activation function and two hidden layers\n",
    "# with 10000 nodes and 4000 noges, and finally an output layer of size\n",
    "# 32*32*3, is instantiated.\n",
    "\n",
    "# a method for the forward pass is also created, which first flattens the\n",
    "# input image and then performs a forward pass of the image through the Generator.\n",
    "# this method then returns the output 'fake' image produced by the generator.\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 10000),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(10000, 4000),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(4000, image_dim),\n",
    "            # Tanh() normalizes the output\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        # flatten the input image and perform a forward pass\n",
    "        out = self.linear(noise)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the Generator and Discriminator, their optimizers, and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the models and assign them to the device available\n",
    "discr = Discriminator(image_dim=image_dim).to(device)\n",
    "gen = Generator(noise_dim=noise_dim).to(device)\n",
    "\n",
    "# initialize the optimizers for both parameters. here we use SGD\n",
    "opt_d = optim.Adam(discr.parameters(), lr=lr)\n",
    "opt_g = optim.Adam(gen.parameters(), lr=lr)\n",
    "\n",
    "# initialize the loss function. the paper uses BCE (Binary Cross Entropy)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. The tutorial I am following trains the whole GAN (discriminator + generator) for 500 epochs. For each epoch of the whole GAN, the discriminator is trained for 4 epochs and the generator will be trained for 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.set_printoptions(threshold=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loop for the overall GAN training (500 epochs).\n",
    "# 'total' is used to specify the total number of expected\n",
    "# iterations (https://www.geeksforgeeks.org/python-how-to-make-a-terminal-progress-bar-using-tqdm/)\n",
    "\n",
    "discr_losses = []\n",
    "gen_losses = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), total=num_epochs):\n",
    "\n",
    "    for idx, (real, _) in enumerate(loader):\n",
    "        print(idx)\n",
    "\n",
    "        #############################\n",
    "        # Training the discriminator\n",
    "        #############################\n",
    "\n",
    "        # 'real_img.to(device) performs a Tensor device conversion. in this case,\n",
    "        # it sets the input image, real_img, on the device we previously defined (GPU).\n",
    "        # this also performs the forward pass and returns an output value from the\n",
    "        # sigmoid activation function (a value between 0 and 1).\n",
    "\n",
    "        # this measures the Binary Cross Entropy between the target and the\n",
    "        # input probability. 'out_d1' is the output of the forward pass.\n",
    "        # 'torch.ones((1, 1)).to(device)' is a 1x1 tensor of value 1, which\n",
    "        # stands for the value of the real image (remember, fake=0 & real=1).\n",
    "        # (https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)\n",
    "    \n",
    "\n",
    "        # generate a fake image from the Generator using the input latent space.\n",
    "        # 'detach' is used to return a tensor that is detached from the current\n",
    "        # graph, meaning that the tensor will never require gradient.\n",
    "        # same process as described above, however this time using a fake image\n",
    "        # which is created by the generator.\n",
    "\n",
    "        # Discriminator wants to maximize 'log(D(x)) + log(1 - D(G(z)))'\n",
    "\n",
    "        print('real shape: ', real.shape)                               # [32, 1, 28, 28]\n",
    "        real = real.view(-1, image_dim).to(device)                      # [32, 784]         \n",
    "        discr_real_results = discr(real).view(-1)                       # [32]\n",
    "        \n",
    "        discr_real_loss = criterion(\n",
    "            discr_real_results, torch.ones_like(discr_real_results))\n",
    "        \n",
    "        # create random input noise for the generator\n",
    "        noise = torch.randn(batch_size, noise_dim).to(device)           # [32, 64]\n",
    "        fake_images = gen(noise)                                        # [32, 784]\n",
    "\n",
    "        discr_fake_results = discr(fake_images).view(-1)                # [32]\n",
    "        discr_fake_loss = criterion(\n",
    "            discr_fake_results, torch.zeros_like(discr_fake_results))\n",
    "\n",
    "        discr_loss = (discr_real_loss + discr_fake_loss) / 2\n",
    "        discr.zero_grad()\n",
    "        discr_loss.backward(retain_graph=True)\n",
    "        opt_d.step()\n",
    "\n",
    "\n",
    "        #############################\n",
    "        # Training the generator\n",
    "        #############################\n",
    "\n",
    "        # Generator wants to minimize 'log(1 - D(G(z)))', but a better thing\n",
    "        # to do is to maximize 'log(D(G(z)))', which is equivalent but deals\n",
    "        # better with the vanishing gradient problem\n",
    "        new_discr_fake_results = discr(fake_images).view(-1)\n",
    "        gen_loss = criterion(new_discr_fake_results, torch.ones_like(new_discr_fake_results))\n",
    "        gen.zero_grad()\n",
    "        gen_loss.backward()\n",
    "        opt_d.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the original image and the last fake image generated by the Generator to compare the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('GAN_pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f056265484383e5b66afe0560660179ace645500003f0a42b5b524fe39d5512"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
