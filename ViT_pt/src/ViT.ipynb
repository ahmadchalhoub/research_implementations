{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my personal implementation of the Vision Transformer (https://arxiv.org/pdf/2010.11929v2.pdf). \n",
    "This approach uses the original Transformer architecture but applies it to images.\n",
    "\n",
    "I documented all of my learning process in commits on the 'ViT_dev' branch, and then just merged the final result into 'main' once I was done. If you'd like\n",
    "to see my struggles, please feel free to checkout ViT_dev and enjoy! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize, RandomHorizontalFlip, RandomCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set device and hyperparameters of network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "patch_size = 16         # Patch size (P) = 16\n",
    "latent_size = 768       # Latent vector (D). ViT-Base uses 768\n",
    "n_channels = 3          # Number of channels for input images\n",
    "num_heads = 12          # ViT-Base uses 12 heads\n",
    "num_encoders = 12       # ViT-Base uses 12 encoder layers\n",
    "dropout = 0.1           # Dropout = 0.1 is used with ViT-Base & ImageNet-21k\n",
    "num_classes = 10        # Number of classes in CIFAR10 dataset\n",
    "size = 224              # Size used for training = 224\n",
    "\n",
    "epochs = 30             # Number of epochs\n",
    "base_lr = 10e-3         # Base LR\n",
    "weight_decay = 0.03     # Weight decay for ViT-Base (on ImageNet-21k)\n",
    "batch_size = 4\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and prepare dataset. We will use the CIFAR-10 dataset from PyTorch.\n",
    "The first time you run this, make sure you pass your own 'root' path and use 'download=True' to\n",
    "download the data. Afterwards, you can change the 'download=True' to 'download=False' since the data is already downloaded.\n",
    "\n",
    "The cell below used to prepare the CIFAR dataset is taken almost straight from the PyTorch tutorial on training with CIFAR10: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I resize the input data to 224x224, since that is the training resolution used in the paper.\n",
    "# The mean and std values used to normalize CIFAR10 data is from here: https://github.com/kentaroy47/vision-transformers-cifar10/blob/main/train_cifar10.py\n",
    "transform_training_data = Compose(\n",
    "    [RandomCrop(32, padding=4), Resize((224)), RandomHorizontalFlip(), ToTensor(), Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "train_data = torchvision.datasets.CIFAR10(\n",
    "    root='/home/achalhoub/Desktop/ViT_CIFAR10_data', train=True, download=False, transform=transform_training_data)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to visualize samples from CIFAR10 dataset. This code is copied from: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Input Embedding class. This class performs all the steps needed before the data goes into the ViT's encoder Encoder block. This includes splitting input images into patches, performing the linear projections of the patches, pre-pending a [class] token and adding a position embedding to the linear projection. The output of this class is fed into the Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=patch_size, n_channels=n_channels, device=device, latent_size=latent_size, batch_size=batch_size):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = self.patch_size * self.patch_size * self.n_channels\n",
    "\n",
    "        self.linearProjection = nn.Linear(self.input_size, self.latent_size)\n",
    "\n",
    "        # Random initialization of of [class] token that is prepended to the linear projection vector.\n",
    "        self.class_token = nn.Parameter(torch.randn(self.batch_size, 1, self.latent_size)).to(self.device)\n",
    "\n",
    "        # Positional embedding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(self.batch_size, 1, self.latent_size)).to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, input_data):\n",
    "\n",
    "        input_data = input_data.to(self.device)\n",
    "\n",
    "        # Re-arrange image into patches.\n",
    "        patches = einops.rearrange(\n",
    "            input_data, 'b c (h h1) (w w1) -> b (h w) (h1 w1 c)', h1=self.patch_size, w1=self.patch_size)\n",
    "\n",
    "        linear_projection = self.linearProjection(patches).to(self.device)\n",
    "        b, n, _ = linear_projection.shape\n",
    "\n",
    "        # Prepend the [class] token to the original linear projection\n",
    "        linear_projection = torch.cat((self.class_token, linear_projection), dim=1)\n",
    "        pos_embed = einops.repeat(self.pos_embedding, 'b 1 d -> b m d', m=n+1)\n",
    "\n",
    "        # Add positional embedding to linear projection\n",
    "        linear_projection += pos_embed\n",
    "\n",
    "        return linear_projection\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Encoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, latent_size=latent_size, num_heads=num_heads, device=device, dropout=dropout):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.latent_size = latent_size\n",
    "        self.num_heads = num_heads\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Normalization layer for both sublayers\n",
    "        self.norm = nn.LayerNorm(self.latent_size)\n",
    "        \n",
    "        # Multi-Head Attention layer\n",
    "        self.multihead = nn.MultiheadAttention(\n",
    "            self.latent_size, self.num_heads, dropout=self.dropout)          \n",
    "\n",
    "        # MLP_head layer in the encoder. I use the same configuration as that \n",
    "        # used in the original VitTransformer implementation. The ViT-Base\n",
    "        # variant uses MLP_head size 3072, which is latent_size*4.\n",
    "        self.enc_MLP = nn.Sequential(\n",
    "            nn.Linear(self.latent_size, self.latent_size*4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.latent_size*4, self.latent_size),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, embedded_patches):\n",
    "\n",
    "        # First sublayer: Norm + Multi-Head Attention + residual connection.\n",
    "        # We take the first element ([0]) of the returned output from nn.MultiheadAttention()\n",
    "        # because this module returns 'Tuple[attention_output, attention_output_weights]'. \n",
    "        # Refer to here for more info: https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html \n",
    "        firstNorm_out = self.norm(embedded_patches)\n",
    "        attention_output = self.multihead(firstNorm_out, firstNorm_out, firstNorm_out)[0]\n",
    "\n",
    "        # First residual connection\n",
    "        first_added_output = attention_output + embedded_patches\n",
    "\n",
    "        # Second sublayer: Norm + enc_MLP (Feed forward)\n",
    "        secondNorm_out = self.norm(first_added_output)\n",
    "        ff_output = self.enc_MLP(secondNorm_out)\n",
    "\n",
    "        # Return the output of the second residual connection\n",
    "        return ff_output + first_added_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put together the whole Vision Transformer. What's added to the input embedding layer and the encoder stack here\n",
    "is the output MLP head (MLP_head), which is used for classification at the end of the whole model using the output embedding portion coming from the initial [class] token inserted into the embedding going into the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VitTransformer(nn.Module):\n",
    "    def __init__(self, num_encoders=num_encoders, latent_size=latent_size, device=device, num_classes=num_classes, dropout=dropout):\n",
    "        super(VitTransformer, self).__init__()\n",
    "        self.num_encoders = num_encoders\n",
    "        self.latent_size = latent_size\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = InputEmbedding()\n",
    "\n",
    "        # Create a stack of encoder layers\n",
    "        self.encStack = nn.ModuleList([EncoderBlock() for i in range(self.num_encoders)])\n",
    "\n",
    "        # MLP_head at the classification stage has 'one hidden layer at pre-training time\n",
    "        # and by a single linear layer at fine-tuning time'. For this implementation I will\n",
    "        # use what was used for training, so I'll have a total of two layers, one hidden\n",
    "        # layer and one output layer.\n",
    "        self.MLP_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.latent_size),\n",
    "            nn.Linear(self.latent_size, self.latent_size),\n",
    "            nn.Linear(self.latent_size, self.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, test_input):\n",
    "\n",
    "        # Apply input embedding (patchify + linear projection + position embeding)\n",
    "        # to the input image passed to the model\n",
    "        enc_output = self.embedding(test_input)\n",
    "\n",
    "        # Loop through all the encoder layers\n",
    "        for enc_layer in self.encStack:\n",
    "            enc_output = enc_layer.forward(enc_output)\n",
    "\n",
    "        # Extract the output embedding information of the [class] token\n",
    "        cls_token_embedding = enc_output[:, 0]\n",
    "\n",
    "        # Finally, return the classification vector for all image in the batch\n",
    "        return self.MLP_head(cls_token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VitTransformer(num_encoders, latent_size, device, num_classes).to(device)\n",
    "\n",
    "# Betas used for Adam in paper are 0.9 and 0.999, which are the default in PyTorch\n",
    "optimizer = optim.Adam(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.LinearLR(optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check number of parameters of Vision Transformer model.\n",
    "The output shows 86M parameters, which is the same as that of the ViT-Base variation in the paper (refer to Table 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model.train().to(device)\n",
    "\n",
    "    for epoch in tqdm(range(epochs), total=epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(trainloader)):\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 200 == 0:\n",
    "                print('Batch {} epoch {} has loss = {}'.format(batch_idx, epoch, running_loss/200))                \n",
    "                running_loss = 0\n",
    "\n",
    "        scheduler.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViT_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fdda9396b7482657e401a5eaa831b82214990c744d46f107c541eaad6f4b4b25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
