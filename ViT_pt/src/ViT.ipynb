{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my personal implementation of the Vision VitTransformer (https://arxiv.org/pdf/2010.11929v2.pdf). \n",
    "This approach uses the original VitTransformer architecture but applies it to images.\n",
    "\n",
    "I documented all of my learning process in commits on the 'ViT_dev' branch, and then just merged the final result into 'main' once I was done. If you'd like\n",
    "to see my struggles, please feel free to checkout ViT_dev and enjoy! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize, RandomHorizontalFlip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set device and hyperparameters of network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "patch_size = 8          # Patch size (P)\n",
    "latent_size = 128       # Latent vector (D). ViT-Base uses 768\n",
    "n_channels = 3          # Number of channels for input images\n",
    "num_heads = 2           # ViT-Base uses 12 heads\n",
    "num_encoders = 12       # ViT-Large uses 24 encoder layers\n",
    "dropout = 0.1           # Dropout = 0.1 is used with ViT-Base & ImageNet-21k\n",
    "num_classes = 10        # Number of classes in CIFAR10 dataset\n",
    "\n",
    "epochs = 10             # Number of epochs\n",
    "base_lr = 100e-3         # Base LR\n",
    "weight_decay = 0.03     # Weight decay for ViT-Base (on ImageNet-21k)\n",
    "batch_size = 16\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and prepare dataset. We will use the CIFAR-10 dataset from PyTorch.\n",
    "The firsts time you run this, make sure you pass your own 'root' path and use 'download=True' to\n",
    "download the data. Afterwards, you can change the 'download=True' to 'download=False' since the data is already downloaded.\n",
    "\n",
    "The cell below used to prepare the CIFAR dataset is taken almost straight from the PyTorch tutorial on training with CIFAR10: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I resize the input data to 224x224, since that is the training resolution used in the paper.\n",
    "# The mean and std values used to normalize CIFAR10 data is from here: https://github.com/kuangliu/pytorch-cifar/issues/19\n",
    "transform_training_data = Compose(\n",
    "    [RandomHorizontalFlip(), ToTensor(), Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "train_data = torchvision.datasets.CIFAR10(\n",
    "    root='/home/achalhoub/Desktop/ViT_CIFAR10_data', train=True, download=False, transform=transform_training_data)\n",
    "\n",
    "#test_data = torchvision.datasets.CIFAR10(\n",
    "#    root='/home/achalhoub/Desktop/ViT_CIFAR10_data', train=False, download=False, transform=transform_data)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "#testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to visualize samples from CIFAR10 dataset. This code is from: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Input Embedding class. This class performs all the steps needed before the data goes into the 'VitTransformer Encoder' block. This includes splitting input images into patches, performing the linear projections of the patches, pre-pending a [class] token and adding a position embedding to the linear projection. The output of this class is fed into the Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=patch_size, n_channels=n_channels, device=device, latent_size=latent_size, batch_size=batch_size):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = self.patch_size * self.patch_size * self.n_channels\n",
    "\n",
    "        self.linearProjection = nn.Linear(self.input_size, self.latent_size)\n",
    "\n",
    "        # Random initialization of of [class] token that\n",
    "        # is prepended to the linear projection vector.\n",
    "        self.class_token = nn.Parameter(torch.rand(self.batch_size, self.latent_size).unsqueeze(1)).to(self.device)\n",
    "\n",
    "        # Embedding layer for position embedding.\n",
    "        self.embeddingLayer = nn.Embedding(self.latent_size+1, self.latent_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_data):\n",
    "\n",
    "        input_data = input_data.to(self.device)\n",
    "\n",
    "        # Re-arrange image into patches.\n",
    "        patches = einops.rearrange(\n",
    "            input_data, 'b c (h h1) (w w1) -> b (h w) (h1 w1 c)', h1=self.patch_size, w1=self.patch_size)\n",
    "\n",
    "        linear_projection = self.linearProjection(patches).to(self.device)\n",
    "        \n",
    "        # Prepend the [class] token to the original linear projection\n",
    "        #print('class_token: ', self.class_token.size())\n",
    "        #print('linear_projection: ', linear_projection.size())\n",
    "        new_vector = torch.cat((self.class_token, linear_projection), dim=1)\n",
    "\n",
    "        # Create a positions vector to create the position embedding.\n",
    "        positions_vector = torch.arange(0, new_vector.shape[1]).expand(\n",
    "            new_vector.shape[0], new_vector.shape[1]).to(self.device)\n",
    "\n",
    "        # Add the positional encodings to the linear vector (with the class token)\n",
    "        return new_vector + self.embeddingLayer(positions_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Encoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, latent_size=latent_size, num_heads=num_heads, device=device, dropout=dropout):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.latent_size = latent_size\n",
    "        self.num_heads = num_heads\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # First Normalization layer\n",
    "        self.firstNorm = nn.LayerNorm(self.latent_size)\n",
    "        \n",
    "        # Multi-Head Attention layer\n",
    "        self.multihead = nn.MultiheadAttention(\n",
    "            self.latent_size, self.num_heads, dropout=self.dropout)           \n",
    "        \n",
    "        # Second Normalization layer\n",
    "        self.secondNorm = nn.LayerNorm(self.latent_size) \n",
    "\n",
    "        #self.dropoutLayer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        # MLP_head layer in the encoder. I use the same configuration as that \n",
    "        # used in the original VitTransformer implementation. The ViT-Base\n",
    "        # variant uses MLP_head size 3072, which is latent_size*4.\n",
    "        self.enc_MLP_head = nn.Sequential(\n",
    "            nn.Linear(self.latent_size, self.latent_size*4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.latent_size*4, self.latent_size),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, embedded_patches):\n",
    "\n",
    "        # First sublayer: Norm + Multi-Head Attention + residual connection.\n",
    "        # We take the first element ([0]) of the returned output from nn.MultiheadAttention()\n",
    "        # because this module returns 'Tuple[attention_output, attention_output_weights]'. \n",
    "        # Refer to here for more info: https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html \n",
    "        firstNorm_out = self.firstNorm(embedded_patches)\n",
    "        attention_output = self.multihead.forward(firstNorm_out, firstNorm_out, firstNorm_out)[0]\n",
    "\n",
    "        # First residual connection\n",
    "        first_added_output = attention_output + embedded_patches\n",
    "\n",
    "        #attention_output = self.dropoutLayer(attention_output)\n",
    "\n",
    "        # Second sublayer: Norm + MLP_head (Feed forward)\n",
    "        secondNorm_out = self.secondNorm(first_added_output)\n",
    "        ff_output = self.enc_MLP_head(secondNorm_out)\n",
    "\n",
    "        # Return the output of the second residual connection\n",
    "        return ff_output + first_added_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put together the whole Vision VitTransformer. What's added to the input embedding layer and the encoder stack here\n",
    "is the output MLP head (MLP_head), which is used for classification at the end of the whole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VitTransformer(nn.Module):\n",
    "    def __init__(self, num_encoders=num_encoders, latent_size=latent_size, device=device, num_classes=num_classes):\n",
    "        super(VitTransformer, self).__init__()\n",
    "        self.num_encoders = num_encoders\n",
    "        self.latent_size = latent_size\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.embedding = InputEmbedding()\n",
    "        self.encStack = nn.ModuleList([EncoderBlock() for i in range(self.num_encoders)])\n",
    "\n",
    "        # MLP_head at the classification head has 'one hidden layer at pre-training time\n",
    "        # and by a single linear layer at fine-tuning time'. For this implementation I will\n",
    "        # use what was used for training, so I'll have a total of two layers, one hidden\n",
    "        # layer and one output layer.\n",
    "        self.MLP_head = nn.Sequential(\n",
    "            nn.Linear(self.latent_size, self.latent_size),\n",
    "            nn.Linear(self.latent_size, self.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, test_input):\n",
    "\n",
    "        # Apply input embedding (patchify + linear projection + position embeding)\n",
    "        # to the input image passed to the model\n",
    "        enc_output = self.embedding.forward(test_input)\n",
    "\n",
    "        # Loop through all the encoder layers\n",
    "        for enc_layer in self.encStack:\n",
    "            enc_output = enc_layer.forward(enc_output)\n",
    "\n",
    "        # Finally, take last output from the encoder stack and pass it to\n",
    "        # the MLP Head for classification. Look only at the vector of the first \n",
    "        # token, which is the [class] token that's used for classification. This list\n",
    "        # will have 10 'percentage' values, equivalent to the number of the dataset classes.\n",
    "        # We return a tensor of the following shape (batch_size, num_classes), since that\n",
    "        # is the data format that the loss functions expects as input.\n",
    "        # Refer to here for more details: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss\n",
    "        output = self.MLP_head(enc_output)[:, 0]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_input = torch.randn(3, 224, 224)\n",
    "#print(test_input.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VitTransformer = VitTransformer(num_encoders, latent_size, device, num_classes).to(device)\n",
    "\n",
    "optimizer = optim.Adam(VitTransformer.parameters(), lr=base_lr, betas=(0.9, 0.999))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.LinearLR(optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check number of parameters of Vision Transformer model.\n",
    "The output shows 86M parameters, which is the same as that of the ViT-Base variation in the paper (refer to Table 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary(VitTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    VitTransformer.train()\n",
    "    \n",
    "    #losses = []\n",
    "    for epoch in tqdm(range(epochs), total=epochs):\n",
    "        #running_loss = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(trainloader)):\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = VitTransformer.forward(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #total += targets.size(0)\n",
    "            #correct += torch.argmax(outputs, dim=1).eq(targets).sum().item()\n",
    "            \n",
    "            #accuracy = 100.*correct/total\n",
    "\n",
    "            if batch_idx % 400 == 0:\n",
    "                print('Epoch {} batch_idx {} has loss {}.'.format(epoch, batch_idx, loss.item()))\n",
    "                #print('Epoch {}, batch_idx {} has accuracy {}.'.format(epoch, batch_idx, accuracy))     \n",
    "                #print('targets: ', targets)  \n",
    "                #print('predictions: ', torch.argmax(outputs, dim=1))\n",
    "        #scheduler.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViT_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fdda9396b7482657e401a5eaa831b82214990c744d46f107c541eaad6f4b4b25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
