{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my personal implementation of the Vision Transformer (https://arxiv.org/pdf/2010.11929v2.pdf). \n",
    "This approach uses the original Transformer architecture but applies it to images.\n",
    "\n",
    "I documented alfl of my learning process in commits on the 'ViT_dev' branch, and then just merged the final result into 'main' once I was done. If you'd like\n",
    "to see my struggles, please feel free to checkout ViT_dev and enjoy! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import einops\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, Resize, ToTensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the test image from ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = Image.open('/home/achalhoub/dev/research_implementations/ViT_pt/input/imagenet_sample.jpeg')\n",
    "fig = plt.figure()\n",
    "plt.imshow(test_input)\n",
    "print(test_input.size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set device and hyperparameters of network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "patch_size = 16         # Patch size (P)\n",
    "latent_vector = 1024    # Latent vector (D). ViT-Large uses 1024\n",
    "n_channels = 3          # Number of channels for input images\n",
    "num_heads = 12          # ViT-Large uses 16 heads\n",
    "batch_size = 16\n",
    "num_encoders = 24       # ViT-Large uses 24 encoder layers\n",
    "dropout = 0.1\n",
    "num_classes = 1000      # Number of classes in the dataset used"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Input Embedding class. This class performs all the steps needed before the data goes into the 'Transformer Encoder' block. This includes splitting input images into patches, performing the linear projections of the patches, pre-pending a [class] token and adding a position embedding to the linear projection. The output of this class is fed into the Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=patch_size, n_channels=n_channels, latent_vector=latent_vector):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        self.latent_vector = latent_vector\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.input_size = self.patch_size * self.patch_size * self.n_channels\n",
    "\n",
    "        self.linearProjection = nn.Linear(self.input_size, self.latent_vector)\n",
    "\n",
    "        # Random initialization of of [class] token that\n",
    "        # is prepended to the linear projection\n",
    "        self.class_token = torch.rand(1, self.latent_vector).unsqueeze(0)\n",
    "\n",
    "        # Embedding layer for position encodings\n",
    "        self.embeddingLayer = nn.Embedding(self.latent_vector+1, self.latent_vector)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        image_transform = Compose([Resize((256, 256)), ToTensor()])\n",
    "        transformed_image = image_transform(input).unsqueeze(0)\n",
    "        patches = einops.rearrange(transformed_image, 'b c (h h1) (w w1) -> b (h w) (h1 w1 c)', h1=self.patch_size, w1=self.patch_size)\n",
    "\n",
    "        linear_projection = self.linearProjection(patches)\n",
    "        \n",
    "        # Prepend the [class] token to the original linear projection\n",
    "        new_vector = torch.cat((self.class_token, linear_projection), dim=1)\n",
    "\n",
    "        # Create a positions vector to create the position encodings\n",
    "        positions_vector = torch.arange(0, new_vector.shape[1]).expand(new_vector.shape[0], new_vector.shape[1])\n",
    "\n",
    "        # Add the positional encodings to the linear vector (with the class token)\n",
    "        return new_vector + self.embeddingLayer(positions_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding = InputEmbedding().forward(test_input)\n",
    "print('Embedded Patches dimension: ', test_embedding.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the Encoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, latent_vector=latent_vector, num_heads=num_heads, dropout=dropout):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.latent_vector = latent_vector\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # First Normalization layer\n",
    "        self.firstNorm = nn.LayerNorm(self.latent_vector)       \n",
    "        \n",
    "        # Multi-Head Attention layer\n",
    "        self.multihead = nn.MultiheadAttention(\n",
    "            self.latent_vector, self.num_heads, dropout=self.dropout)              \n",
    "        \n",
    "        # Second Normalization layer\n",
    "        self.secondNorm = nn.LayerNorm(self.latent_vector)      \n",
    "\n",
    "        #self.dropoutLayer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        # MLP layer. I use the same configuration as that used \n",
    "        # in the original Transformer implementation. The ViT paper mentions\n",
    "        # that all layers have a constant D dimension (latent vector size), so\n",
    "        # I'm not sure if they implement expansion here or not.\n",
    "        self.feedForward = nn.Sequential(\n",
    "            nn.Linear(self.latent_vector, self.latent_vector*4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.latent_vector*4, self.latent_vector)\n",
    "        )\n",
    "\n",
    "    def forward(self, embedded_patches):\n",
    "\n",
    "        # First sublayer: Norm + Multi-Head Attention + residual connection\n",
    "        firstNorm_out = self.firstNorm(embedded_patches)\n",
    "        attention_output = self.multihead.forward(firstNorm_out, firstNorm_out, firstNorm_out) \n",
    "        first_added_output = attention_output + embedded_patches\n",
    "\n",
    "        #attention_output = self.dropoutLayer(attention_output)\n",
    "\n",
    "        # Second sublayer: Norm + MLP (Feed forward)\n",
    "        secondNorm_out = self.secondNorm(first_added_output)\n",
    "        ff_output = self.feedForward(secondNorm_out)\n",
    "\n",
    "        #ff_output = self.dropoutLayer(ff_output)\n",
    "\n",
    "        # Return the output of the second residual connection\n",
    "        return ff_output + first_added_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output MLP block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_encoders=num_encoders, latent_vector=latent_vector, device=device, num_classes=num_classes):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.num_encoders = num_encoders\n",
    "        self.latent_vector = latent_vector\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.embedding = InputEmbedding().to(self.device)\n",
    "        self.encStack = nn.ModuleList([EncoderBlock() for i in range(self.num_encoders)])\n",
    "\n",
    "        # MLP at the classification head has 'one hidden layer at pre-training time'.\n",
    "        # I'll have to double-check if what I've implemented is correct.\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(self.latent_vector, self.latent_vector),\n",
    "            nn.Linear(self.latent_vector, self.num_classes)\n",
    "        )\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, test_input, test_target):\n",
    "\n",
    "        enc_output = self.embedding.forward(test_input)\n",
    "\n",
    "        # Final output 'enc_output' of this loop will be both the key and value\n",
    "        # that will be taken as input to the second sub-layer of the decoder\n",
    "        for enc_layer in self.encStack:\n",
    "            enc_output = enc_layer.forward(enc_output, enc_output, enc_output)\n",
    "\n",
    "        # Pass the final decoder stack output to the linear layer that takes in\n",
    "        # input vector of size 'latent_vector' and outputs a vector that has the \n",
    "        # size of the vocab specified. Finall return the softmax output of that vector\n",
    "        #final_output = self.finalLinear(dec_output)\n",
    "\n",
    "        return self.softmax(final_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViT_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fdda9396b7482657e401a5eaa831b82214990c744d46f107c541eaad6f4b4b25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
