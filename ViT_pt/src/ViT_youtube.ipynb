{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing packages needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize, RandomHorizontalFlip, RandomCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters of the network and specify device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "patch_size = 16\n",
    "latent_size = 768\n",
    "n_channels = 3\n",
    "num_heads = 12\n",
    "num_encoders = 12\n",
    "dropout = 0.1\n",
    "num_classes = 10\n",
    "size = 224\n",
    "\n",
    "epochs = 10\n",
    "base_lr = 10e-3 \n",
    "weight_decay = 0.03\n",
    "batch_size = 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of input linear projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=patch_size, n_channels=n_channels, device=device, latent_size=latent_size, batch_size=batch_size):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = self.patch_size*self.patch_size*self.n_channels\n",
    "\n",
    "        # Linear projection\n",
    "        self.linearProjection = nn.Linear(self.input_size, self.latent_size)\n",
    "\n",
    "        # Class token\n",
    "        self.class_token = nn.Parameter(torch.randn(self.batch_size, 1, self.latent_size)).to(self.device)\n",
    "\n",
    "        # Positional embedding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(self.batch_size, 1, self.latent_size)).to(self.device)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        input_data = input_data.to(self.device)\n",
    "\n",
    "        # Patchify input image\n",
    "        patches = einops.rearrange(\n",
    "            input_data, 'b c (h h1) (w w1) -> b (h w) (h1 w1 c)', h1=self.patch_size, w1=self.patch_size)\n",
    "\n",
    "        #print(input_data.size())\n",
    "        #print(patches.size())\n",
    "        \n",
    "        linear_projection = self.linearProjection(patches).to(self.device)\n",
    "        b, n, _ = linear_projection.shape\n",
    "\n",
    "        linear_projection = torch.cat((self.class_token, linear_projection), dim=1)\n",
    "        pos_embed = einops.repeat(self.pos_embedding, 'b 1 d -> b m d', m=n+1)\n",
    "    \n",
    "        linear_projection += pos_embed\n",
    "\n",
    "        return linear_projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randn((8, 3, 224, 224))\n",
    "test_class = InputEmbedding().to(device)\n",
    "embed_test = test_class(test_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Encoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, latent_size=latent_size, num_heads=num_heads, device=device, dropout=dropout):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.latent_size = latent_size\n",
    "        self.num_heads = num_heads\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Normalization layer\n",
    "        self.norm = nn.LayerNorm(self.latent_size)\n",
    "\n",
    "        self.multihead = nn.MultiheadAttention(\n",
    "            self.latent_size, self.num_heads, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        self.enc_MLP = nn.Sequential(\n",
    "            nn.Linear(self.latent_size, self.latent_size*4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.latent_size*4, self.latent_size),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, embedded_patches):\n",
    "        firstnorm_out = self.norm(embedded_patches)\n",
    "        attention_out = self.multihead(firstnorm_out, firstnorm_out, firstnorm_out)[0]\n",
    "\n",
    "        # first residual connection\n",
    "        first_added = attention_out + embedded_patches\n",
    "\n",
    "        secondnorm_out = self.norm(first_added)\n",
    "        ff_out = self.enc_MLP(secondnorm_out)\n",
    "\n",
    "        return ff_out + first_added\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.2355e-01, -1.8078e+00,  1.1759e+00,  ..., -3.4359e-01,\n",
       "          -7.2185e-02,  1.3529e+00],\n",
       "         [ 4.6247e-02, -9.9380e-01, -4.5251e-01,  ..., -1.1911e+00,\n",
       "           1.4419e+00,  1.3693e+00],\n",
       "         [ 9.7330e-01, -1.8508e+00,  3.5775e-01,  ..., -1.4842e+00,\n",
       "           1.3984e+00,  1.1508e+00],\n",
       "         ...,\n",
       "         [ 4.4001e-01, -2.2045e+00,  3.7167e-01,  ..., -1.4944e+00,\n",
       "           6.3034e-01,  7.8634e-01],\n",
       "         [ 8.0298e-01, -1.7168e+00,  9.0895e-01,  ..., -2.4058e+00,\n",
       "           1.4339e+00,  3.6456e-01],\n",
       "         [ 1.2904e+00, -1.6537e+00,  5.2323e-01,  ..., -2.5036e+00,\n",
       "           5.7349e-02,  1.3578e+00]],\n",
       "\n",
       "        [[ 1.4517e+00, -6.3832e-01, -1.6566e+00,  ...,  2.2055e+00,\n",
       "          -2.2559e+00,  5.0645e-01],\n",
       "         [-1.2711e-02, -2.3971e+00, -1.5669e+00,  ...,  1.5312e+00,\n",
       "          -8.7514e-01, -1.2052e-01],\n",
       "         [-4.3736e-01, -2.1193e+00, -1.3339e-01,  ...,  1.6818e+00,\n",
       "          -1.0663e+00,  6.4820e-01],\n",
       "         ...,\n",
       "         [ 1.3788e-02, -1.8526e+00, -1.2668e+00,  ...,  1.7691e+00,\n",
       "          -1.8914e+00,  2.5618e-01],\n",
       "         [ 2.6238e-01, -2.0529e+00,  3.5680e-01,  ...,  1.4683e+00,\n",
       "          -1.5602e+00,  1.1357e-01],\n",
       "         [ 1.6207e-01, -1.6125e+00, -1.3866e+00,  ...,  2.2617e+00,\n",
       "          -5.6807e-01,  1.1339e+00]],\n",
       "\n",
       "        [[-1.2672e+00, -1.4856e-01,  1.5602e+00,  ...,  2.1991e+00,\n",
       "          -1.2486e+00, -1.7170e-01],\n",
       "         [-3.7478e-01, -6.4317e-01, -3.2742e-01,  ...,  5.9062e-01,\n",
       "           1.6437e-01, -1.0952e+00],\n",
       "         [ 9.9148e-01,  1.3252e+00,  9.6202e-01,  ...,  1.9828e+00,\n",
       "           1.2022e+00,  2.9229e-01],\n",
       "         ...,\n",
       "         [ 3.7088e-01, -2.6928e-01, -9.2409e-02,  ...,  1.6224e+00,\n",
       "           6.1120e-01, -8.9764e-01],\n",
       "         [ 1.5168e-01, -5.8141e-01,  5.3248e-01,  ...,  7.8025e-01,\n",
       "          -4.5713e-01, -9.9387e-01],\n",
       "         [-4.2612e-02, -1.0160e-01,  4.4339e-01,  ...,  1.1321e+00,\n",
       "           4.7833e-01, -2.9934e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.7329e+00, -5.1510e-01, -1.3044e+00,  ...,  3.9115e-01,\n",
       "          -1.1873e+00,  1.4893e-02],\n",
       "         [ 1.7062e+00,  2.0492e-02, -1.7004e+00,  ...,  4.0649e-01,\n",
       "          -2.1908e-01,  1.2993e+00],\n",
       "         [ 1.2185e+00,  5.6975e-01, -2.4015e+00,  ...,  7.5577e-01,\n",
       "           1.2131e-01,  1.8912e+00],\n",
       "         ...,\n",
       "         [ 8.3602e-01, -2.7638e-01, -2.4368e+00,  ...,  5.4059e-01,\n",
       "          -1.3308e-01,  7.9779e-01],\n",
       "         [ 1.1355e+00,  5.3998e-01, -1.5306e+00,  ...,  1.0412e+00,\n",
       "          -9.8864e-02,  9.7996e-01],\n",
       "         [ 6.7134e-01,  2.9268e-01, -1.3705e+00,  ...,  5.9457e-01,\n",
       "           9.1425e-03,  1.0559e+00]],\n",
       "\n",
       "        [[ 1.0229e+00, -1.3397e+00, -9.1159e-01,  ...,  5.4501e-01,\n",
       "          -1.2581e+00,  7.0177e-01],\n",
       "         [ 9.2465e-01,  7.0487e-01,  5.0102e-01,  ...,  6.1266e-01,\n",
       "           8.0824e-02,  5.6624e-01],\n",
       "         [ 1.4588e+00,  6.5031e-01,  1.0828e+00,  ...,  9.2771e-01,\n",
       "           5.2106e-01,  1.4890e+00],\n",
       "         ...,\n",
       "         [ 7.7599e-01,  1.1045e+00,  5.2800e-02,  ...,  7.6491e-01,\n",
       "          -4.6249e-02,  4.1133e-01],\n",
       "         [-4.9481e-01,  3.4648e-01,  4.6090e-01,  ..., -8.3455e-02,\n",
       "          -4.3700e-01,  6.7889e-01],\n",
       "         [ 1.8109e+00,  1.0031e-01,  5.4800e-01,  ..., -5.8352e-02,\n",
       "          -2.4797e-01,  7.3459e-01]],\n",
       "\n",
       "        [[ 7.2107e-01, -4.7173e-01,  1.1589e+00,  ...,  2.9735e-01,\n",
       "           1.4880e+00, -7.0142e-01],\n",
       "         [ 1.6501e+00, -3.5470e-01,  1.3571e+00,  ..., -1.4331e-01,\n",
       "           2.8422e+00, -8.6004e-01],\n",
       "         [ 1.6338e+00,  8.4834e-01,  2.0173e+00,  ..., -2.4022e-01,\n",
       "           2.2190e+00, -8.0860e-01],\n",
       "         ...,\n",
       "         [ 1.9637e+00,  1.6625e-01,  1.4186e+00,  ..., -1.1565e+00,\n",
       "           2.1986e+00, -2.4150e-01],\n",
       "         [ 1.4228e+00,  6.0784e-01,  1.0354e+00,  ...,  1.8243e-01,\n",
       "           2.4201e+00, -1.8745e-01],\n",
       "         [ 1.9886e+00, -1.7574e+00,  1.2424e+00,  ...,  1.1122e-03,\n",
       "           1.7630e+00, -4.5249e-01]]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoder = EncoderBlock().to(device)\n",
    "test_encoder(embed_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vit(nn.Module):\n",
    "    def __init__(self, num_encoders=num_encoders, latent_size=latent_size, device=device, num_classes=num_classes, dropout=dropout):\n",
    "        super(Vit, self).__init__()\n",
    "        self.num_encoder = num_encoders\n",
    "        self.latent_size = latent_size\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = InputEmbedding()\n",
    "\n",
    "        # Create the stack of encoders\n",
    "        self.encStack = nn.ModuleList([EncoderBlock() for i in range(self.num_encoder)])\n",
    "\n",
    "        self.MLP_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.latent_size),\n",
    "            nn.Linear(self.latent_size, self.latent_size),\n",
    "            nn.Linear(self.latent_size, self.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, test_input):\n",
    "        enc_output = self.embedding(test_input)\n",
    "\n",
    "        for enc_layer in self.encStack:\n",
    "            enc_output = enc_layer(enc_output)\n",
    "\n",
    "        cls_token_embed = enc_output[:, 0]\n",
    "\n",
    "        return self.MLP_head(cls_token_embed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3911,  0.3709, -0.0763,  0.3983,  0.0033,  0.1565, -0.2159,  0.3850,\n",
      "         -0.0957, -0.2416],\n",
      "        [-0.9099, -0.6156, -0.4327,  0.2852, -0.3389,  0.4391,  0.0589,  0.1776,\n",
      "          0.2711,  0.2731],\n",
      "        [-0.4315, -0.3108, -0.3195, -0.1811,  0.1486,  0.3366,  0.2288, -0.6239,\n",
      "          0.2557,  0.0992],\n",
      "        [-0.0902,  0.2124, -0.3800,  0.0741,  0.0971, -0.0359, -0.1254,  0.1438,\n",
      "          0.1679, -0.1064],\n",
      "        [ 0.3950,  0.1897, -0.0706,  0.3142, -0.2319, -0.5109, -0.6775, -0.2189,\n",
      "         -0.5458,  0.4139],\n",
      "        [-0.6946,  0.0742, -0.0423,  0.4796, -0.0975,  0.0267, -0.2221, -0.2862,\n",
      "          0.0536, -0.4352],\n",
      "        [-0.4652,  0.3042, -0.1481,  0.3951, -0.0936,  0.3667, -0.0196, -0.0049,\n",
      "         -0.1430,  0.0348],\n",
      "        [-0.2570, -0.1593, -0.2347,  0.2582,  0.0360,  0.1477, -0.3275, -0.1828,\n",
      "          0.4856, -0.1503]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 10])\n"
     ]
    }
   ],
   "source": [
    "model = Vit().to(device)\n",
    "vit_output = model(test_input)\n",
    "print(vit_output)\n",
    "print(vit_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViT_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fdda9396b7482657e401a5eaa831b82214990c744d46f107c541eaad6f4b4b25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
