{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achalhoub/miniconda3/envs/transformer_pt/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import spacy\n",
    "\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load English and German tokenizers using Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_lang = spacy.load(\"en_core_web_sm\")\n",
    "ger_lang = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify device as GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a single batch of random input just to use to check if the shape initializations in the below classes are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test input has shape (N, seq_len, embed_len) => ((8, 20, 512)).\n",
    "# I decreased the batch size from 64 to 8 because of memory issues of my GPU.\n",
    "# 64 was too big for the memory, and the biggest size that worked was 8.\n",
    "test_query = torch.rand((8, 20, 512)).to(device)\n",
    "test_key = torch.rand((8, 20, 512)).to(device)\n",
    "test_value = torch.rand((8, 20, 512)).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters. I used the same ones as those used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "d_model = 512"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin building the Transformer. The first step is to build the 'Scaled Dot-Product Attention' block mentioned in the paper. This is still just the first draft; it will probably need some fixes once I get to later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProduct(nn.Module):\n",
    "    def __init__(self, queries):\n",
    "        super(ScaledDotProduct, self).__init__()\n",
    "        \n",
    "        # I probably don't need to initialize K, Q, and V here, since they will be passed to the \n",
    "        # scaled dot product when we call it from the MultiHeadAttention class in the forward method.\n",
    "        # Will delete the variable below later if I turn out to be right.\n",
    "        self.queries = queries\n",
    "        #self.keys = keys\n",
    "        #self.values = values\n",
    "        self.dk = self.queries.shape[1]\n",
    "        #print('self.dk type: ', type(self.dk))\n",
    "\n",
    "        # Softmax operator. 'dim' still needs to be specified\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    # Define the forward function\n",
    "    def forward(self, queries, keys, values):\n",
    "        compatibility = torch.bmm(queries, torch.transpose(keys, 1, 2))   # first batch MatMul operation\n",
    "        compatibility = compatibility / math.sqrt((self.dk))             # scaling down by sqrt(dk)\n",
    "        compatibility_softmax = self.softmax(compatibility)               # normalizing using Softmax\n",
    "        output = torch.bmm(compatibility_softmax, values)                 # final batch MatMul operation\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the 'Multi-Head Attention' block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, queries, keys, values):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = h\n",
    "        self.batch_num = queries.shape[0]\n",
    "        self.seq_len = queries.shape[1]\n",
    "        self.embed_len = queries.shape[2]\n",
    "        self.d_model = d_model\n",
    "        self.queries = queries\n",
    "        self.keys = keys\n",
    "        self.values = values\n",
    "        self.head_length = self.d_model/self.num_heads\n",
    "\n",
    "        self.concat_output = []\n",
    "\n",
    "        # For an input, Q for example, which would originally have a shape\n",
    "        # of (N, seq_len, embed_len), it would be split up into the number of \n",
    "        # heads that we define (ex: 8). So, the new shape would be\n",
    "        # (N, seq_len, embed_len/8). This would also apply to K and V too.\n",
    "\n",
    "        # Since we are flattening batches of matrices, I'm not sure if the flattening\n",
    "        # should be done in another way. I'll come back to this later if it needs changing.\n",
    "        self.q_in = (torch.flatten(self.queries, start_dim=1, end_dim=2) / self.num_heads).shape[1]        \n",
    "        self.k_in = (torch.flatten(self.keys, start_dim=1, end_dim=2) / self.num_heads).shape[1]\n",
    "        self.v_in = (torch.flatten(self.values, start_dim=1, end_dim=2) / self.num_heads).shape[1]\n",
    "        \n",
    "        \n",
    "        # For the input of each Linear layer, we would have the divided Q, K, \n",
    "        # and V calculated above. q_in, k_in, and v_in = 10240 each.\n",
    "        print(type(self.q_in))\n",
    "        print(self.q_in)\n",
    "        self.q_linear = nn.Linear(int(self.q_in/8), int(self.q_in/8))\n",
    "        self.k_linear = nn.Linear(int(self.k_in/8), int(self.k_in/8))\n",
    "        self.v_linear = nn.Linear(int(self.v_in/8), int(self.v_in/8))\n",
    "\n",
    "        # Attention layer.\n",
    "        self.attention = ScaledDotProduct(self.queries)\n",
    "\n",
    "        # This is the final Linear layer, after the outputs of all the heads\n",
    "        # from the Scaled Dot Product layer have been concatenated together. The\n",
    "        # output dimension of this layer is a hyperparameter that we define. Here\n",
    "        # we use d_model, which is 512.\n",
    "        self.output_linear = nn.Linear(self.q_in, self.d_model)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # Feed the 8 heads of Q, K, and V into the linear layers in parallel, and then into the\n",
    "        # attention block. Let's say the original tensor Q has the following shape: \n",
    "        # (N, seq_len, embed_len) -> (8, 20, 512).\n",
    "        # The segment that will go into each head will be of the following size:\n",
    "        # (N, seq_len, embed_len/num_heads) -> (8, 20, 64). So we need to slice the third dimension.\n",
    "        for i in range(self.num_heads):\n",
    "\n",
    "            # The output of each of the linear layers has length -> (N, seq_len*embed_len/num_heads) -> (N, 1280)\n",
    "            q_linear_output = self.q_linear(torch.flatten(queries[:, :, int(i*self.head_length):int((i+1)*self.head_length)], start_dim=1, end_dim=2))\n",
    "            k_linear_output = self.k_linear(torch.flatten(keys[:, :, int(i*self.head_length):int((i+1)*self.head_length)], start_dim=1, end_dim=2))\n",
    "            v_linear_output = self.v_linear(torch.flatten(values[:, :, int(i*self.head_length):int((i+1)*self.head_length)], start_dim=1, end_dim=2))\n",
    "\n",
    "            #print('q_linear_output shape: ', q_linear_output.shape)\n",
    "\n",
    "            # Since the three outputs computed from the linear layers above are just 1D vectors of length\n",
    "            # (N, seq_len*embed_len/num_heads) -> (N, 1280), and the ScaledDotProduct forward method expects 3D tensors,\n",
    "            # I will reshape the 1D vectors into 3D tensors of shape (N, seq_len, embed_len/num_heads)\n",
    "            q_reshaped_output = torch.reshape(q_linear_output, (self.batch_num, self.seq_len, int(self.embed_len/self.num_heads)))\n",
    "            k_reshaped_output = torch.reshape(k_linear_output, (self.batch_num, self.seq_len, int(self.embed_len/self.num_heads)))\n",
    "            v_reshaped_output = torch.reshape(v_linear_output, (self.batch_num, self.seq_len, int(self.embed_len/self.num_heads)))\n",
    "\n",
    "            #print('q_reshaped_output shape: ', q_reshaped_output.shape)\n",
    "\n",
    "            # Feed reshaped Q, K, and V into ScaledDotProduct layer.\n",
    "            # 'sdp_output' should have shape (N, seq_len, embed_len/num_heads)\n",
    "            sdp_output = self.attention.forward(q_reshaped_output, k_reshaped_output, v_reshaped_output)\n",
    "\n",
    "            # Each 'sdp_output' is a Tensor of shape (N, seq_len, embed_len/num_heads) -> (8, 20, 64).\n",
    "            # Each flattened Tensor has length (8, 20*64) = 10240\n",
    "            #print('sdp_output shape: ', sdp_output.shape)\n",
    "            #print('sdp_output flattened length: ', torch.flatten(sdp_output, start_dim=1, end_dim=2).shape)\n",
    "            \n",
    "            # We need to concatenate the outputs of all the heads\n",
    "            # into one vector and pass it through a final linear layer\n",
    "            self.concat_output.append(torch.flatten(sdp_output, start_dim=1, end_dim=2))\n",
    "            \n",
    "        flattened_concat_output = torch.flatten(torch.stack(self.concat_output), start_dim=1, end_dim=2)\n",
    "        \n",
    "        # Pass the concatenated vector in a final linear layer and return output\n",
    "        return self.output_linear(flattened_concat_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "10240\n"
     ]
    }
   ],
   "source": [
    "multihead = MultiHeadAttention(num_heads, d_model, test_query, test_key, test_value).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_concat_output shape:  torch.Size([8, 10240])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12286/2582992481.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  compatibility_softmax = self.softmax(compatibility)               # normalizing using Softmax\n"
     ]
    }
   ],
   "source": [
    "test_output = multihead.forward(test_query, test_key, test_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe19fb5bed516e6ac74a9b255f69d3cfb16eeb4b5a81b7a48fdeb18da73b6471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
