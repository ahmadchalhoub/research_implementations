{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import spacy\n",
    "\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load English and German tokenizers using Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_lang = spacy.load(\"en_core_web_sm\")\n",
    "ger_lang = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify device as GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a single batch of random input just to use to check if the shape initializations in the below classes are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters. I used the same ones as those used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "embed_len = 512\n",
    "seq_len = 20        # just a dummy number for now\n",
    "batch_size = 8      # just a dummy number for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.rand((batch_size, seq_len, embed_len)).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin building the Transformer. The first step is to build the 'Scaled Dot-Product Attention' block mentioned in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProduct(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len, mask=None):\n",
    "        super(ScaledDotProduct, self).__init__()\n",
    "        \n",
    "        self.dk = embed_len                 # dk = embed_len\n",
    "        self.mask = mask\n",
    "        self.softmax = nn.Softmax(dim=2)    # Softmax operator\n",
    "\n",
    "    # Define the forward function\n",
    "    def forward(self, queries, keys, values):       \n",
    "        compatibility = torch.bmm(queries, torch.transpose(keys, 1, 2))    # first batch MatMul operation\n",
    "        compatibility = compatibility / math.sqrt((self.dk))               # scaling down by sqrt(dk)  -> (8, 20, 20)\n",
    "\n",
    "        # Apply mask after scaling the result of MatMul of Q and K.\n",
    "        # This is needed in the decoder to prevent the decoder from\n",
    "        # 'peaking ahead' and knowing what word will come next.\n",
    "        # Check: https://pytorch.org/docs/stable/generated/torch.tril.html \n",
    "        if self.mask is not None:\n",
    "            compatibility = torch.tril(compatibility)\n",
    "            \n",
    "        compatibility_softmax = self.softmax(compatibility)                # normalizing using Softmax -> (8, 20, 512) \n",
    "        return torch.bmm(compatibility_softmax, values)                    # final batch MatMul operation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the 'Multi-Head Attention' block. Init variable need simplifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads=num_heads, embed_len=embed_len, batch_size=batch_size, seq_len=seq_len, mask=None):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_len = embed_len\n",
    "        self.head_length = self.embed_len/self.num_heads\n",
    "        self.mask = mask\n",
    "        self.concat_output = []\n",
    "\n",
    "        # For an input, Q for example, which would originally have a shape\n",
    "        # of (N, seq_len, embed_len), it would be split up into the number of \n",
    "        # heads that we define (ex: 8). So, the new shape would be\n",
    "        # (N, seq_len, embed_len/8). This would also apply to K and V too.\n",
    "\n",
    "        # Since we are flattening batches of matrices, I'm not sure if the flattening\n",
    "        # should be done in another way. I'll come back to this later if it needs changing.\n",
    "        self.q_in = self.k_in = self.v_in = self.seq_len * self.embed_len       # 20*512 = 10240\n",
    "\n",
    "        # For the input of each Linear layer, we would have the divided Q, K, \n",
    "        # and V calculated above. q_in, k_in, and v_in = 10240 each. So each\n",
    "        # linear will have input size = output size = 10240/8 = 1280.\n",
    "        self.q_linear = nn.Linear(int(self.q_in/8), int(self.q_in/8))\n",
    "        self.k_linear = nn.Linear(int(self.k_in/8), int(self.k_in/8))\n",
    "        self.v_linear = nn.Linear(int(self.v_in/8), int(self.v_in/8))\n",
    "\n",
    "        # Attention layer.\n",
    "        if self.mask is not None:\n",
    "            self.attention = ScaledDotProduct(mask=True) \n",
    "        else:\n",
    "            self.attention = ScaledDotProduct()\n",
    "\n",
    "        # This is the final Linear layer, after the outputs of all the heads\n",
    "        # from the Scaled Dot Product layer have been concatenated together. The\n",
    "        # output dimension of this layer is a hyperparameter that we define. Here\n",
    "        # we use embed_len, which is 512.\n",
    "        self.output_linear = nn.Linear(self.q_in, self.embed_len)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # Feed the 8 heads of Q, K, and V into the linear layers in parallel, and then into the\n",
    "        # attention block. Let's say the original tensor Q has the following shape: \n",
    "        # (N, seq_len, embed_len) -> (8, 20, 512).\n",
    "        # The segment that will go into each head will be of the following size:\n",
    "        # (N, seq_len, embed_len/num_heads) -> (8, 20, 64). So we need to slice the third dimension.\n",
    "        for i in range(self.num_heads):\n",
    "\n",
    "            # The output of each of the linear layers has length -> (N, seq_len*embed_len/num_heads) -> (N, 1280)\n",
    "            q_linear_output = self.q_linear(torch.flatten(queries[:, :, int(i*self.head_length):int((i+1)*self.head_length)], start_dim=1, end_dim=2))\n",
    "            k_linear_output = self.k_linear(torch.flatten(keys[:, :, int(i*self.head_length):int((i+1)*self.head_length)], start_dim=1, end_dim=2))\n",
    "            v_linear_output = self.v_linear(torch.flatten(values[:, :, int(i*self.head_length):int((i+1)*self.head_length)], start_dim=1, end_dim=2))\n",
    "\n",
    "            # Since the three outputs computed from the linear layers above are just 1D vectors of length\n",
    "            # (N, seq_len*embed_len/num_heads) -> (N, 1280), and the ScaledDotProduct forward method expects 3D tensors,\n",
    "            # I will reshape the 1D vectors into 3D tensors of shape (N, seq_len, embed_len/num_heads)\n",
    "            q_reshaped_output = torch.reshape(q_linear_output, (self.batch_size, self.seq_len, int(self.embed_len/self.num_heads)))\n",
    "            k_reshaped_output = torch.reshape(k_linear_output, (self.batch_size, self.seq_len, int(self.embed_len/self.num_heads)))\n",
    "            v_reshaped_output = torch.reshape(v_linear_output, (self.batch_size, self.seq_len, int(self.embed_len/self.num_heads)))\n",
    "\n",
    "            #print('q_reshaped_output shape: ', q_reshaped_output.shape)\n",
    "\n",
    "            # Feed reshaped Q, K, and V into ScaledDotProduct layer.\n",
    "            # 'sdp_output' should have shape (N, seq_len, embed_len/num_heads)\n",
    "            sdp_output = self.attention.forward(q_reshaped_output, k_reshaped_output, v_reshaped_output)\n",
    "\n",
    "            # Each 'sdp_output' is a Tensor of shape (N, seq_len, embed_len/num_heads) -> (8, 20, 64).\n",
    "            # Each flattened Tensor has length (8, 20*64) = 10240\n",
    "            #print('sdp_output shape: ', sdp_output.shape)\n",
    "            #print('sdp_output flattened length: ', torch.flatten(sdp_output, start_dim=1, end_dim=2).shape)\n",
    "            \n",
    "            # We need to concatenate the outputs of all the heads\n",
    "            # into one vector and pass it through a final linear layer\n",
    "            self.concat_output.append(torch.flatten(sdp_output, start_dim=1, end_dim=2))\n",
    "            \n",
    "        flattened_concat_output = torch.flatten(torch.stack(self.concat_output), start_dim=1, end_dim=2)\n",
    "        \n",
    "        # Pass the concatenated vector in a final linear layer and return output\n",
    "        return self.output_linear(flattened_concat_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead = MultiHeadAttention(mask=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = multihead.forward(test_input, test_input, test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_output.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Encoder block. I will then stack this into multiple layers to create the Encoder stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.embed_len = embed_len\n",
    "        self.multihead = MultiHeadAttention()       # Multi-Head Attention layer\n",
    "        self.firstNorm = nn.LayerNorm(embed_len)    # Normalization layer (after the multi-head attention layer)\n",
    "        self.secondNorm = nn.LayerNorm(embed_len)   # Normalization layer (after the Feed Forward layer)\n",
    "\n",
    "        # The Feed Forward layer. In the paper this has input &\n",
    "        # output = 512 (or = embed_len) and inner-layer = 2048 (or = embed_len*4)\n",
    "        self.feedForward = nn.Sequential(\n",
    "            nn.Linear(embed_len, embed_len*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_len*4, embed_len)\n",
    "        )\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        attention_output = self.multihead.forward(queries, keys, values)\n",
    "        # the output of the first residual connection\n",
    "        first_sublayer_output = self.firstNorm(attention_output + queries)\n",
    "\n",
    "        ff_output = self.feedForward(first_sublayer_output)\n",
    "\n",
    "        # return the output of the second residual connection\n",
    "        return self.secondNorm(ff_output + first_sublayer_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Decoder block. I will also stack this into multiple layers to create the Decoder stack.\n",
    "\n",
    "The decoder has a total of 3 inputs: the queries, which come from the previous decoder layer, and the memory keys and values, which come from the output of the encoder (Section 3.2.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        # Masked Multi-Head Attention and Normalization layers.\n",
    "        self.maskedMultihead = MultiHeadAttention(mask=True)\n",
    "        self.firstNorm = nn.LayerNorm(embed_len)\n",
    "\n",
    "        # The output of the above two layers and the output from the encoder stack feed \n",
    "        # into an 'encoder block'\n",
    "        self.encoderBlock = EncoderBlock()\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "\n",
    "        # First sublayer, which consists of the Masked Multi-Head Attention + Normalization\n",
    "        # sublayer, with a residual connection\n",
    "        masked_multihead_output = self.maskedMultihead.forward(queries, queries, queries)\n",
    "        first_sublayer_output = self.firstNorm(masked_multihead_output + queries)\n",
    "\n",
    "        # The remaining of the DecoderBlock is basically an encoder block, which takes keys \n",
    "        # and values from the actual Encoder stack output, and takes queries from the \n",
    "        # previous sublayer of the DecoderBlock\n",
    "        return self.encoderBlock.forward(first_sublayer_output, keys, values)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe19fb5bed516e6ac74a9b255f69d3cfb16eeb4b5a81b7a48fdeb18da73b6471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
