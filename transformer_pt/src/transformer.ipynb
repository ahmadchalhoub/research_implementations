{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and extract the training, validation, and test data for English and German from the Multi30k dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand downloaded data. What format of train, val, and test data were downloaded exactly, and how much data is in it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train file paths: ', train_filepaths)\n",
    "print('Val file paths: ', val_filepaths)\n",
    "print('Test file paths: ', test_filepaths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, there is 6 files in total: 2 training files, 2 validation files, and 2 test files. Now to check what is in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open training files\n",
    "german_train_file = open(train_filepaths[0], \"r\")\n",
    "english_train_file = open(train_filepaths[1], \"r\")\n",
    "\n",
    "# Check length of german and english training data\n",
    "german_train_length = len(german_train_file.readlines())\n",
    "english_train_length = len(english_train_file.readlines())\n",
    "print('German train sentences length: ', german_train_length)\n",
    "print('English train sentences lenght: ', english_train_length)\n",
    "\n",
    "german_train_file.seek(0)\n",
    "english_train_file.seek(0)\n",
    "\n",
    "# Visualize first 2 sentences from english and german data\n",
    "print(german_train_file.readlines()[0:2])\n",
    "print(english_train_file.readlines()[0:2])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same steps for val and test data. From the above output, we know we have 29,000 training samples. Repeating the steps for val and test below shows that we have 1014 validation samples and 1000 testing samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open val files\n",
    "german_val_file = open(val_filepaths[0], \"r\")\n",
    "english_val_file = open(val_filepaths[1], \"r\")\n",
    "\n",
    "# Check length of german and english val data\n",
    "german_val_length = len(german_val_file.readlines())\n",
    "english_val_length = len(english_val_file.readlines())\n",
    "print('German val sentences length: ', german_val_length)\n",
    "print('English val sentences lenght: ', english_val_length)\n",
    "\n",
    "german_val_file.seek(0)\n",
    "english_val_file.seek(0)\n",
    "\n",
    "# Visualize first 2 sentences from english and german val data\n",
    "print(german_val_file.readlines()[0:2])\n",
    "print(english_val_file.readlines()[0:2])\n",
    "\n",
    "# Open test files\n",
    "german_test_file = open(test_filepaths[0], \"r\")\n",
    "english_test_file = open(test_filepaths[1], \"r\")\n",
    "\n",
    "# Check length of german and english test data\n",
    "german_test_length = len(german_test_file.readlines())\n",
    "english_test_length = len(english_test_file.readlines())\n",
    "print('German val sentences length: ', german_test_length)\n",
    "print('English val sentences lenght: ', english_test_length)\n",
    "\n",
    "german_test_file.seek(0)\n",
    "english_test_file.seek(0)\n",
    "\n",
    "# Visualize first 2 sentences from english and german test data\n",
    "print(german_test_file.readlines()[0:2])\n",
    "print(english_test_file.readlines()[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_train_file.seek(0)\n",
    "english_train_file.seek(0)\n",
    "german_val_file.seek(0)\n",
    "english_val_file.seek(0)\n",
    "german_test_file.seek(0)\n",
    "english_test_file.seek(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load English and German tokenizers using Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_lang = spacy.load(\"en_core_web_sm\")\n",
    "ger_lang = spacy.load(\"de_core_news_sm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe19fb5bed516e6ac74a9b255f69d3cfb16eeb4b5a81b7a48fdeb18da73b6471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
