{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my personal implementation of the Transformer architecture, introduced by Google in 2017 (https://arxiv.org/pdf/1706.03762.pdf).\n",
    "The Transformer architecture is a revolutionary advancement that really changed the state of machine learning language models, and I am currently writing\n",
    "this during the same day when Google just announced Bard, their own 'ChatGPT' that uses their LaMBDA model (very exciting stuff)! \n",
    "\n",
    "Working on this has been a lot of fun, and I've learned a ton, which is my main goal of doing this. I probably have mistakes (hopefully minor and not major)\n",
    "in the implementation, so if you notice anything that should've been done differently, please do reach out and let me know! You can reach me at chalhoah@gmail.com.\n",
    "\n",
    "I mostly worked on everything myself without getting assistance from other implementations, but I did get stuck on some spots, and when I did,\n",
    "the following resources were super helpful and educational for me:\n",
    "- https://www.youtube.com/watch?v=U0s0f995w14&t=1593s\n",
    "- https://github.com/tunz/transformer-pytorch\n",
    "- http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "I documented all of my learning process in commits on the 'develop' branch, and then just merged the final result into 'main' once I was done. If you'd like\n",
    "to see my struggles, please feel free to checkout develop and enjoy! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achalhoub/miniconda3/envs/transformer_pt/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify device as GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters. I used the same ones as those used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "embed_len = 512\n",
    "batch_size = 8              # chosen batch size\n",
    "stack_len = 6               # length of encoder and decoder stacks (=6 as used in paper)\n",
    "dropout = 0.1               # dropout value to use\n",
    "\n",
    "output_vocab_size = 7000    # just a dummy number\n",
    "input_vocab_size = 7000     # just a dummy number"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Input embedding class. This includes both the normal embedding for the tokenized input sequences, as well as the positional embeddings. They are added together and the sum of them is used as the input embedding to the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, input_vocab_size=input_vocab_size, embed_len=embed_len, dropout=dropout, device=device):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.embed_len = embed_len\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.firstEmbedding = nn.Embedding(self.input_vocab_size, self.embed_len)\n",
    "        self.secondEmbedding = nn.Embedding(self.input_vocab_size, self.embed_len)\n",
    "\n",
    "        self.dropoutLayer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        first_embedding = self.firstEmbedding(input)\n",
    "        \n",
    "        batch_size, seq_len = input.shape\n",
    "\n",
    "        positions_vector = torch.arange(0, seq_len).expand(batch_size, seq_len).to(self.device)\n",
    "        second_embedding = self.secondEmbedding(positions_vector)\n",
    "\n",
    "        return self.dropoutLayer(first_embedding + second_embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin building the Transformer. The first step is to build the 'Scaled Dot-Product Attention' block mentioned in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProduct(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len, mask=None):\n",
    "        super(ScaledDotProduct, self).__init__()\n",
    "        \n",
    "        self.dk = embed_len                 # dk = embed_len\n",
    "        self.mask = mask\n",
    "        self.softmax = nn.Softmax(dim=3)    # Softmax operator\n",
    "\n",
    "    # Define the forward function\n",
    "    def forward(self, queries, keys, values):       \n",
    "\n",
    "        # First batch MatMul operation & scaling down by sqrt(dk).\n",
    "        # Output 'compatibility' has shape:\n",
    "        # (batch_size, num_heads, seq_len, seq_len)\n",
    "        compatibility = torch.matmul(queries, torch.transpose(keys, 2, 3)) \n",
    "        compatibility = compatibility / math.sqrt((self.dk))               \n",
    "\n",
    "        # Apply mask after scaling the result of MatMul of Q and K.\n",
    "        # This is needed in the decoder to prevent the decoder from\n",
    "        # 'peaking ahead' and knowing what word will come next.\n",
    "        # Check: https://pytorch.org/docs/stable/generated/torch.tril.html \n",
    "        if self.mask is not None:\n",
    "            compatibility = torch.tril(compatibility)\n",
    "            \n",
    "        # Normalize using Softmax\n",
    "        compatibility_softmax = self.softmax(compatibility)        \n",
    "               \n",
    "        return torch.matmul(compatibility_softmax, torch.transpose(values, 1, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the 'Multi-Head Attention' block. Init variable need simplifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads=num_heads, embed_len=embed_len, batch_size=batch_size, mask=None):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.batch_size = batch_size\n",
    "        self.embed_len = embed_len\n",
    "        self.head_length = int(self.embed_len/self.num_heads)\n",
    "        self.mask = mask\n",
    "        self.concat_output = []\n",
    "\n",
    "        # Q, K, and V have shape: (batch_size, seq_len, embed_len)\n",
    "        self.q_in = self.k_in = self.v_in = self.embed_len\n",
    "\n",
    "        # Linear layers take in embed_len as input \n",
    "        # dim and produce embed_len as output dim\n",
    "        self.q_linear = nn.Linear(int(self.q_in), int(self.q_in))\n",
    "        self.k_linear = nn.Linear(int(self.k_in), int(self.k_in))\n",
    "        self.v_linear = nn.Linear(int(self.v_in), int(self.v_in))\n",
    "\n",
    "        # Attention layer.\n",
    "        if self.mask is not None:\n",
    "            self.attention = ScaledDotProduct(mask=True) \n",
    "        else:\n",
    "            self.attention = ScaledDotProduct()\n",
    "\n",
    "        self.output_linear = nn.Linear(self.q_in, self.embed_len)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "\n",
    "        # Query has shape: (batch_size, seq_len, num_heads, head_length)\n",
    "        # Then transpose it: (batch_size, num_heads, seq_len, head_length)\n",
    "        queries = self.q_linear(queries).reshape(\n",
    "            self.batch_size, -1, self.num_heads, self.head_length)\n",
    "        queries = queries.transpose(1, 2)\n",
    "\n",
    "        # Same for Key as for Query above.\n",
    "        keys = self.k_linear(keys).reshape(\n",
    "            self.batch_size, -1, self.num_heads, self.head_length)\n",
    "        keys = keys.transpose(1, 2)\n",
    "\n",
    "        # Value has shape: (batch_size, seq_len, num_heads, head_length)\n",
    "        values = self.v_linear(values).reshape(\n",
    "            self.batch_size, -1, self.num_heads, self.head_length)\n",
    "\n",
    "        # 'sdp_output' here has size: \n",
    "        # (batch_size, num_heads, seq_len, head_length)\n",
    "        sdp_output = self.attention.forward(queries, keys, values)\n",
    "\n",
    "        # Reshape to (batch_size, seq_len, num_heads*head_length)\n",
    "        sdp_output = sdp_output.transpose(1, 2).reshape(\n",
    "            self.batch_size, -1, self.num_heads * self.head_length)\n",
    "\n",
    "        # Return self.output_linear(sdp_output).\n",
    "        # This has shape (batch_size, seq_len, embed_len)\n",
    "        return self.output_linear(sdp_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Encoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len, dropout=dropout):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.embed_len = embed_len\n",
    "        self.dropout = dropout\n",
    "        self.multihead = MultiHeadAttention()             # Multi-Head Attention layer\n",
    "        self.firstNorm = nn.LayerNorm(embed_len)          # Normalization layer (after the multi-head attention layer)\n",
    "        self.secondNorm = nn.LayerNorm(embed_len)         # Normalization layer (after the Feed Forward layer)\n",
    "        self.dropoutLayer = nn.Dropout(p=self.dropout)    # Dropout layer (before addition and normalization)\n",
    "\n",
    "        # The Feed Forward layer. In the paper this has input &\n",
    "        # output = 512 (or = embed_len) and inner-layer = 2048 (or = embed_len*4)\n",
    "        self.feedForward = nn.Sequential(\n",
    "            nn.Linear(embed_len, embed_len*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_len*4, embed_len)\n",
    "        )\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        attention_output = self.multihead.forward(queries, keys, values)\n",
    "        attention_output = self.dropoutLayer(attention_output)\n",
    "\n",
    "        # the output of the first residual connection\n",
    "        first_sublayer_output = self.firstNorm(attention_output + queries)\n",
    "\n",
    "        ff_output = self.feedForward(first_sublayer_output)\n",
    "        ff_output = self.dropoutLayer(ff_output)\n",
    "\n",
    "        # return the output of the second residual connection\n",
    "        return self.secondNorm(ff_output + first_sublayer_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Decoder block.\n",
    "\n",
    "The decoder has a total of 3 inputs: the queries, which come from the previous decoder layer, and the memory keys and values, which come from the output of the encoder (Section 3.2.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len, dropout=dropout):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.embed_len = embed_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Masked Multi-Head Attention and Normalization layers.\n",
    "        self.maskedMultihead = MultiHeadAttention(mask=True)\n",
    "        self.firstNorm = nn.LayerNorm(self.embed_len)\n",
    "\n",
    "        self.dropoutLayer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        # The output of the above two layers and the output from the encoder stack feed \n",
    "        # into an 'encoder block'\n",
    "        self.encoderBlock = EncoderBlock()\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "\n",
    "        # First sublayer, which consists of the Masked Multi-Head Attention + Normalization\n",
    "        # sublayer, with a residual connection\n",
    "        masked_multihead_output = self.maskedMultihead.forward(queries, queries, queries)\n",
    "        masked_multihead_output = self.dropoutLayer(masked_multihead_output)\n",
    "        first_sublayer_output = self.firstNorm(masked_multihead_output + queries)\n",
    "\n",
    "        # The remaining of the DecoderBlock is basically an encoder block, which takes keys \n",
    "        # and values from the actual Encoder stack output, and takes queries from the \n",
    "        # previous sublayer of the DecoderBlock\n",
    "        return self.encoderBlock.forward(first_sublayer_output, keys, values)      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create whole Transformer block (still missing some components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, stack_len=stack_len, embed_len=embed_len, device=device, output_vocab_size=output_vocab_size):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.stack_len = stack_len\n",
    "        self.embed_len = embed_len\n",
    "        self.device = device\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "\n",
    "        self.embedding = InputEmbedding().to(self.device)\n",
    "        self.encStack = nn.ModuleList([EncoderBlock() for i in range(self.stack_len)])\n",
    "        self.decStack = nn.ModuleList([DecoderBlock() for i in range(self.stack_len)])\n",
    "        self.finalLinear = nn.Linear(self.embed_len, self.output_vocab_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, test_input, test_target):\n",
    "\n",
    "        enc_output = self.embedding.forward(test_input)\n",
    "\n",
    "        # Final output 'enc_output' of this loop will be both the key and value\n",
    "        # that will be taken as input to the second sub-layer of the decoder\n",
    "        for enc_layer in self.encStack:\n",
    "            enc_output = enc_layer.forward(enc_output, enc_output, enc_output)\n",
    "\n",
    "        # Decoder stack will take the 'enc_output' from the decoder as the keys\n",
    "        # and values, and will take its own output from the previous layer as\n",
    "        # the query. The query used for the first layer is the '<sos>' token.\n",
    "        dec_output = self.embedding(test_target)\n",
    "        for dec_layer in self.decStack:\n",
    "            dec_output = dec_layer.forward(dec_output, enc_output, enc_output)\n",
    "\n",
    "        # Pass the final decoder stack output to the linear layer that takes in\n",
    "        # input vector of size 'embed_len' and outputs a vector that has the \n",
    "        # size of the vocab specified. Finall return the softmax output of that vector\n",
    "        final_output = self.finalLinear(dec_output)\n",
    "\n",
    "        return self.softmax(final_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with random example input and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 30, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14741/475266861.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.softmax(final_output)\n"
     ]
    }
   ],
   "source": [
    "input_tokens = torch.randint(10, (batch_size, 30)).to(device)\n",
    "output_target = torch.randint(10, (batch_size, 20)).to(device)\n",
    "\n",
    "Embedding = InputEmbedding().to(device)\n",
    "input_embeddings = Embedding.forward(input_tokens).to(device)\n",
    "\n",
    "transformer = Transformer().to(device)\n",
    "print(input_embeddings.shape)\n",
    "\n",
    "transformer_output = transformer.forward(input_tokens, output_target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen below, the final output of the softmax layer is -> (batch size, sequence length, output_vocab_size) -> (8, 20, 37000) in our sample numbers.\n",
    "Let's consider that we are only working with one sample (batch_size=1) and ignore the first dimension. The remaining dimensions are the output sequence length and the output vocab size. This is basically saying that for every output token, there is a vector [1, output_vocab_size] that provides the probability for every work in the output vocabulary language. The model chooses the highest probability as the next word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 20, 7000])\n"
     ]
    }
   ],
   "source": [
    "print(transformer_output.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe19fb5bed516e6ac74a9b255f69d3cfb16eeb4b5a81b7a48fdeb18da73b6471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
