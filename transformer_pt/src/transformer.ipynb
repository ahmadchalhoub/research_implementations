{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-Do:\n",
    "- Import WMT dataset and perform actual training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achalhoub/miniconda3/envs/transformer_pt/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import spacy\n",
    "\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load English and German tokenizers using Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_lang = spacy.load(\"en_core_web_sm\")\n",
    "ger_lang = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify device as GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a single batch of random input just to use to check if the shape initializations in the below classes are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters. I used the same ones as those used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "embed_len = 512\n",
    "seq_len = 20                # just a dummy number for now\n",
    "batch_size = 8              # just a dummy number for now\n",
    "stack_len = 6               # length of encoder and decoder stacks (=6 as used in paper)\n",
    "output_vocab_size = 4000    # just a dummy number for now\n",
    "dropout = 0.1\n",
    "input_vocab_size = 37000    # just a dummy number for now (from paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.rand((batch_size, seq_len, embed_len)).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Input embedding class. This includes both the normal embedding for the tokenized input sequences, as well as the positional embeddings. They are added together and the sum of them is used as the input embedding to the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, input_vocab_size=input_vocab_size, embed_len=embed_len, dropout=dropout, device=device):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.embed_len = embed_len\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.firstEmbedding = nn.Embedding(self.input_vocab_size, self.embed_len)\n",
    "        self.secondEmbedding = nn.Embedding(self.input_vocab_size, self.embed_len)\n",
    "\n",
    "        self.dropoutLayer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        first_embedding = self.firstEmbedding(input)\n",
    "        \n",
    "        batch_size, seq_len = input.shape\n",
    "\n",
    "        positions_vector = torch.arange(0, seq_len).expand(batch_size, seq_len).to(self.device)\n",
    "        second_embedding = self.secondEmbedding(positions_vector)\n",
    "\n",
    "        return self.dropoutLayer(first_embedding + second_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens = torch.randint(10, (8, 20)).to(device)\n",
    "input_tokens.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first embedding size:  torch.Size([8, 20, 512])\n",
      "second embedding size:  torch.Size([8, 20, 512])\n",
      "first embedding:  tensor([[[-0.6014,  0.8802, -1.0483,  ..., -0.4807, -0.3084,  0.0236],\n",
      "         [-0.1844,  1.3597, -2.2082,  ..., -0.5663,  0.6736, -0.9453],\n",
      "         [-0.6014,  0.8802, -1.0483,  ..., -0.4807, -0.3084,  0.0236],\n",
      "         ...,\n",
      "         [-0.1844,  1.3597, -2.2082,  ..., -0.5663,  0.6736, -0.9453],\n",
      "         [-1.4538, -1.0109, -0.4769,  ...,  0.3788, -0.5659, -0.6634],\n",
      "         [-0.5754, -1.5874,  1.5591,  ...,  0.2740,  1.0448, -0.2669]],\n",
      "\n",
      "        [[-1.4602, -0.9560,  0.1756,  ...,  1.0818,  0.2341,  0.1611],\n",
      "         [ 0.5136, -2.6123, -0.2758,  ..., -1.7128,  0.4511, -1.0418],\n",
      "         [-1.3922, -0.1399,  0.3916,  ..., -0.9458,  1.0900,  1.4927],\n",
      "         ...,\n",
      "         [-0.6418, -0.7586, -1.7455,  ..., -0.3027,  0.5802,  0.4377],\n",
      "         [-0.3961, -1.1131, -0.4194,  ...,  0.0183, -0.4295, -0.8628],\n",
      "         [ 0.5136, -2.6123, -0.2758,  ..., -1.7128,  0.4511, -1.0418]],\n",
      "\n",
      "        [[-1.4602, -0.9560,  0.1756,  ...,  1.0818,  0.2341,  0.1611],\n",
      "         [-0.3961, -1.1131, -0.4194,  ...,  0.0183, -0.4295, -0.8628],\n",
      "         [ 0.5136, -2.6123, -0.2758,  ..., -1.7128,  0.4511, -1.0418],\n",
      "         ...,\n",
      "         [-0.6014,  0.8802, -1.0483,  ..., -0.4807, -0.3084,  0.0236],\n",
      "         [ 0.5136, -2.6123, -0.2758,  ..., -1.7128,  0.4511, -1.0418],\n",
      "         [-1.4602, -0.9560,  0.1756,  ...,  1.0818,  0.2341,  0.1611]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3961, -1.1131, -0.4194,  ...,  0.0183, -0.4295, -0.8628],\n",
      "         [-1.4538, -1.0109, -0.4769,  ...,  0.3788, -0.5659, -0.6634],\n",
      "         [ 0.5136, -2.6123, -0.2758,  ..., -1.7128,  0.4511, -1.0418],\n",
      "         ...,\n",
      "         [-1.4602, -0.9560,  0.1756,  ...,  1.0818,  0.2341,  0.1611],\n",
      "         [-1.4538, -1.0109, -0.4769,  ...,  0.3788, -0.5659, -0.6634],\n",
      "         [-1.4602, -0.9560,  0.1756,  ...,  1.0818,  0.2341,  0.1611]],\n",
      "\n",
      "        [[ 0.5849,  0.2151,  0.3691,  ..., -1.2240,  0.5639,  0.1417],\n",
      "         [-1.4602, -0.9560,  0.1756,  ...,  1.0818,  0.2341,  0.1611],\n",
      "         [-0.6418, -0.7586, -1.7455,  ..., -0.3027,  0.5802,  0.4377],\n",
      "         ...,\n",
      "         [-1.4538, -1.0109, -0.4769,  ...,  0.3788, -0.5659, -0.6634],\n",
      "         [-0.6014,  0.8802, -1.0483,  ..., -0.4807, -0.3084,  0.0236],\n",
      "         [-1.4538, -1.0109, -0.4769,  ...,  0.3788, -0.5659, -0.6634]],\n",
      "\n",
      "        [[-0.6014,  0.8802, -1.0483,  ..., -0.4807, -0.3084,  0.0236],\n",
      "         [-0.1844,  1.3597, -2.2082,  ..., -0.5663,  0.6736, -0.9453],\n",
      "         [-0.1844,  1.3597, -2.2082,  ..., -0.5663,  0.6736, -0.9453],\n",
      "         ...,\n",
      "         [-1.4602, -0.9560,  0.1756,  ...,  1.0818,  0.2341,  0.1611],\n",
      "         [-0.6014,  0.8802, -1.0483,  ..., -0.4807, -0.3084,  0.0236],\n",
      "         [-1.4538, -1.0109, -0.4769,  ...,  0.3788, -0.5659, -0.6634]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "second embedding:  tensor([[[ 0.2313, -0.3950, -0.3112,  ..., -0.9863,  1.7159, -2.2867],\n",
      "         [ 0.5883,  1.4191,  0.0397,  ...,  1.4544,  0.5127, -0.2637],\n",
      "         [ 0.7889, -1.1642,  0.6236,  ..., -0.1241, -0.3325,  2.3169],\n",
      "         ...,\n",
      "         [ 0.1694, -0.9455,  0.7606,  ..., -0.7993,  0.1090,  0.4540],\n",
      "         [-1.2323, -0.3245,  0.6052,  ...,  1.0400, -1.5255, -0.1471],\n",
      "         [ 0.5371, -0.5087,  0.4310,  ..., -0.7684,  1.7764,  0.0927]],\n",
      "\n",
      "        [[ 0.2313, -0.3950, -0.3112,  ..., -0.9863,  1.7159, -2.2867],\n",
      "         [ 0.5883,  1.4191,  0.0397,  ...,  1.4544,  0.5127, -0.2637],\n",
      "         [ 0.7889, -1.1642,  0.6236,  ..., -0.1241, -0.3325,  2.3169],\n",
      "         ...,\n",
      "         [ 0.1694, -0.9455,  0.7606,  ..., -0.7993,  0.1090,  0.4540],\n",
      "         [-1.2323, -0.3245,  0.6052,  ...,  1.0400, -1.5255, -0.1471],\n",
      "         [ 0.5371, -0.5087,  0.4310,  ..., -0.7684,  1.7764,  0.0927]],\n",
      "\n",
      "        [[ 0.2313, -0.3950, -0.3112,  ..., -0.9863,  1.7159, -2.2867],\n",
      "         [ 0.5883,  1.4191,  0.0397,  ...,  1.4544,  0.5127, -0.2637],\n",
      "         [ 0.7889, -1.1642,  0.6236,  ..., -0.1241, -0.3325,  2.3169],\n",
      "         ...,\n",
      "         [ 0.1694, -0.9455,  0.7606,  ..., -0.7993,  0.1090,  0.4540],\n",
      "         [-1.2323, -0.3245,  0.6052,  ...,  1.0400, -1.5255, -0.1471],\n",
      "         [ 0.5371, -0.5087,  0.4310,  ..., -0.7684,  1.7764,  0.0927]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2313, -0.3950, -0.3112,  ..., -0.9863,  1.7159, -2.2867],\n",
      "         [ 0.5883,  1.4191,  0.0397,  ...,  1.4544,  0.5127, -0.2637],\n",
      "         [ 0.7889, -1.1642,  0.6236,  ..., -0.1241, -0.3325,  2.3169],\n",
      "         ...,\n",
      "         [ 0.1694, -0.9455,  0.7606,  ..., -0.7993,  0.1090,  0.4540],\n",
      "         [-1.2323, -0.3245,  0.6052,  ...,  1.0400, -1.5255, -0.1471],\n",
      "         [ 0.5371, -0.5087,  0.4310,  ..., -0.7684,  1.7764,  0.0927]],\n",
      "\n",
      "        [[ 0.2313, -0.3950, -0.3112,  ..., -0.9863,  1.7159, -2.2867],\n",
      "         [ 0.5883,  1.4191,  0.0397,  ...,  1.4544,  0.5127, -0.2637],\n",
      "         [ 0.7889, -1.1642,  0.6236,  ..., -0.1241, -0.3325,  2.3169],\n",
      "         ...,\n",
      "         [ 0.1694, -0.9455,  0.7606,  ..., -0.7993,  0.1090,  0.4540],\n",
      "         [-1.2323, -0.3245,  0.6052,  ...,  1.0400, -1.5255, -0.1471],\n",
      "         [ 0.5371, -0.5087,  0.4310,  ..., -0.7684,  1.7764,  0.0927]],\n",
      "\n",
      "        [[ 0.2313, -0.3950, -0.3112,  ..., -0.9863,  1.7159, -2.2867],\n",
      "         [ 0.5883,  1.4191,  0.0397,  ...,  1.4544,  0.5127, -0.2637],\n",
      "         [ 0.7889, -1.1642,  0.6236,  ..., -0.1241, -0.3325,  2.3169],\n",
      "         ...,\n",
      "         [ 0.1694, -0.9455,  0.7606,  ..., -0.7993,  0.1090,  0.4540],\n",
      "         [-1.2323, -0.3245,  0.6052,  ...,  1.0400, -1.5255, -0.1471],\n",
      "         [ 0.5371, -0.5087,  0.4310,  ..., -0.7684,  1.7764,  0.0927]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[[-0.3700,  0.4852, -1.3595,  ..., -1.4669,  1.4075, -2.2631],\n",
      "         [ 0.4039,  2.7788, -2.1685,  ...,  0.8881,  1.1864, -1.2090],\n",
      "         [ 0.1875, -0.2840, -0.4246,  ..., -0.6047, -0.6409,  2.3404],\n",
      "         ...,\n",
      "         [-0.0150,  0.4142, -1.4476,  ..., -1.3656,  0.7827, -0.4913],\n",
      "         [-2.6860, -1.3353,  0.1283,  ...,  1.4188, -2.0913, -0.8105],\n",
      "         [-0.0383, -2.0962,  1.9902,  ..., -0.4945,  2.8213, -0.1742]],\n",
      "\n",
      "        [[-1.2289, -1.3510, -0.1357,  ...,  0.0956,  1.9500, -2.1256],\n",
      "         [ 1.1019, -1.1932, -0.2361,  ..., -0.2584,  0.9639, -1.3055],\n",
      "         [-0.6034, -1.3041,  1.0152,  ..., -1.0699,  0.7575,  3.8096],\n",
      "         ...,\n",
      "         [-0.4724, -1.7040, -0.9849,  ..., -1.1019,  0.6892,  0.8917],\n",
      "         [-1.6283, -1.4375,  0.1858,  ...,  1.0583, -1.9550, -1.0099],\n",
      "         [ 1.0507, -3.1210,  0.1552,  ..., -2.4812,  2.2276, -0.9491]],\n",
      "\n",
      "        [[-1.2289, -1.3510, -0.1357,  ...,  0.0956,  1.9500, -2.1256],\n",
      "         [ 0.1923,  0.3060, -0.3797,  ...,  1.4728,  0.0833, -1.1265],\n",
      "         [ 1.3024, -3.7765,  0.3479,  ..., -1.8369,  0.1186,  1.2751],\n",
      "         ...,\n",
      "         [-0.4320, -0.0653, -0.2876,  ..., -1.2799, -0.1994,  0.4775],\n",
      "         [-0.7187, -2.9367,  0.3294,  ..., -0.6728, -1.0743, -1.1889],\n",
      "         [-0.9231, -1.4647,  0.6066,  ...,  0.3134,  2.0105,  0.2538]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1647, -1.5080, -0.7306,  ..., -0.9679,  1.2865, -3.1495],\n",
      "         [-0.8655,  0.4082, -0.4372,  ...,  1.8333, -0.0531, -0.9270],\n",
      "         [ 1.3024, -3.7765,  0.3479,  ..., -1.8369,  0.1186,  1.2751],\n",
      "         ...,\n",
      "         [-1.2908, -1.9015,  0.9362,  ...,  0.2826,  0.3431,  0.6151],\n",
      "         [-2.6860, -1.3353,  0.1283,  ...,  1.4188, -2.0913, -0.8105],\n",
      "         [-0.9231, -1.4647,  0.6066,  ...,  0.3134,  2.0105,  0.2538]],\n",
      "\n",
      "        [[ 0.8162, -0.1799,  0.0579,  ..., -2.2103,  2.2798, -2.1450],\n",
      "         [-0.8719,  0.4631,  0.2152,  ...,  2.5363,  0.7468, -0.1026],\n",
      "         [ 0.1471, -1.9228, -1.1219,  ..., -0.4267,  0.2477,  2.7546],\n",
      "         ...,\n",
      "         [-1.2844, -1.9563,  0.2838,  ..., -0.4204, -0.4568, -0.2094],\n",
      "         [-1.8336,  0.5557, -0.4431,  ...,  0.5593, -1.8339, -0.1236],\n",
      "         [-0.9167, -1.5196, -0.0459,  ..., -0.3896,  1.2106, -0.5707]],\n",
      "\n",
      "        [[-0.3700,  0.4852, -1.3595,  ..., -1.4669,  1.4075, -2.2631],\n",
      "         [ 0.4039,  2.7788, -2.1685,  ...,  0.8881,  1.1864, -1.2090],\n",
      "         [ 0.6045,  0.1955, -1.5846,  ..., -0.6903,  0.3411,  1.3716],\n",
      "         ...,\n",
      "         [-1.2908, -1.9015,  0.9362,  ...,  0.2826,  0.3431,  0.6151],\n",
      "         [-1.8336,  0.5557, -0.4431,  ...,  0.5593, -1.8339, -0.1236],\n",
      "         [-0.9167, -1.5196, -0.0459,  ..., -0.3896,  1.2106, -0.5707]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_embedding = InputEmbedding().to(device)\n",
    "print(test_embedding.forward(input_tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin building the Transformer. The first step is to build the 'Scaled Dot-Product Attention' block mentioned in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProduct(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len, mask=None):\n",
    "        super(ScaledDotProduct, self).__init__()\n",
    "        \n",
    "        self.dk = embed_len                 # dk = embed_len\n",
    "        self.mask = mask\n",
    "        self.softmax = nn.Softmax(dim=2)    # Softmax operator\n",
    "\n",
    "    # Define the forward function\n",
    "    def forward(self, queries, keys, values):       \n",
    "        compatibility = torch.bmm(queries, torch.transpose(keys, 1, 2))    # first batch MatMul operation\n",
    "        compatibility = compatibility / math.sqrt((self.dk))               # scaling down by sqrt(dk)  -> (8, 20, 20)\n",
    "\n",
    "        # Apply mask after scaling the result of MatMul of Q and K.\n",
    "        # This is needed in the decoder to prevent the decoder from\n",
    "        # 'peaking ahead' and knowing what word will come next.\n",
    "        # Check: https://pytorch.org/docs/stable/generated/torch.tril.html \n",
    "        if self.mask is not None:\n",
    "            compatibility = torch.tril(compatibility)\n",
    "            \n",
    "        compatibility_softmax = self.softmax(compatibility)                # normalizing using Softmax -> (8, 20, 512) \n",
    "        return torch.bmm(compatibility_softmax, values)                    # final batch MatMul operation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the 'Multi-Head Attention' block. Init variable need simplifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads=num_heads, embed_len=embed_len, batch_size=batch_size, seq_len=seq_len, mask=None):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_len = embed_len\n",
    "        self.head_length = self.embed_len/self.num_heads\n",
    "        self.mask = mask\n",
    "        self.concat_output = []\n",
    "\n",
    "        # For an input, Q for example, which would originally have a shape\n",
    "        # of (N, seq_len, embed_len), it would be split up into the number of \n",
    "        # heads that we define (ex: 8). So, the new shape would be\n",
    "        # (N, seq_len, embed_len/8). This would also apply to K and V too.\n",
    "\n",
    "        # Since we are flattening batches of matrices, I'm not sure if the flattening\n",
    "        # should be done in another way. I'll come back to this later if it needs changing.\n",
    "        self.q_in = self.k_in = self.v_in = self.seq_len * self.embed_len       # 20*512 = 10240\n",
    "\n",
    "        # For the input of each Linear layer, we would have the divided Q, K, \n",
    "        # and V calculated above. q_in, k_in, and v_in = 10240 each. So each\n",
    "        # linear will have input size = output size = 10240/8 = 1280.\n",
    "        self.q_linear = nn.Linear(int(self.q_in/8), int(self.q_in/8))\n",
    "        self.k_linear = nn.Linear(int(self.k_in/8), int(self.k_in/8))\n",
    "        self.v_linear = nn.Linear(int(self.v_in/8), int(self.v_in/8))\n",
    "\n",
    "        # Attention layer.\n",
    "        if self.mask is not None:\n",
    "            self.attention = ScaledDotProduct(mask=True) \n",
    "        else:\n",
    "            self.attention = ScaledDotProduct()\n",
    "\n",
    "        # This is the final Linear layer, after the outputs of all the heads\n",
    "        # from the Scaled Dot Product layer have been concatenated together. The\n",
    "        # output dimension of this layer is a hyperparameter that we define. Here\n",
    "        # we use embed_len, which is 512.\n",
    "        self.output_linear = nn.Linear(self.q_in, self.embed_len)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # Feed the 8 heads of Q, K, and V into the linear layers in parallel, and then into the\n",
    "        # attention block. Let's say the original tensor Q has the following shape: \n",
    "        # (N, seq_len, embed_len) -> (8, 20, 512).\n",
    "        # The segment that will go into each head will be of the following size:\n",
    "        # (N, seq_len, embed_len/num_heads) -> (8, 20, 64). So we need to slice the third dimension.\n",
    "        for i in range(self.num_heads):\n",
    "\n",
    "            # The output of each of the linear layers has length -> (N, seq_len*embed_len/num_heads) -> (N, 1280)\n",
    "            q_linear_output = self.q_linear(torch.flatten(queries[:, :, int(i*self.head_length):int((i+1)*self.head_length)], start_dim=1, end_dim=2))\n",
    "            k_linear_output = self.k_linear(torch.flatten(keys[:, :, int(i*self.head_length):int((i+1)*self.head_length)], start_dim=1, end_dim=2))\n",
    "            v_linear_output = self.v_linear(torch.flatten(values[:, :, int(i*self.head_length):int((i+1)*self.head_length)], start_dim=1, end_dim=2))\n",
    "\n",
    "            # Since the three outputs computed from the linear layers above are just 1D vectors of length\n",
    "            # (N, seq_len*embed_len/num_heads) -> (N, 1280), and the ScaledDotProduct forward method expects 3D tensors,\n",
    "            # I will reshape the 1D vectors into 3D tensors of shape (N, seq_len, embed_len/num_heads)\n",
    "            q_reshaped_output = torch.reshape(q_linear_output, (self.batch_size, self.seq_len, int(self.embed_len/self.num_heads)))\n",
    "            k_reshaped_output = torch.reshape(k_linear_output, (self.batch_size, self.seq_len, int(self.embed_len/self.num_heads)))\n",
    "            v_reshaped_output = torch.reshape(v_linear_output, (self.batch_size, self.seq_len, int(self.embed_len/self.num_heads)))\n",
    "\n",
    "            #print('q_reshaped_output shape: ', q_reshaped_output.shape)\n",
    "\n",
    "            # Feed reshaped Q, K, and V into ScaledDotProduct layer.\n",
    "            # 'sdp_output' should have shape (N, seq_len, embed_len/num_heads)\n",
    "            sdp_output = self.attention.forward(q_reshaped_output, k_reshaped_output, v_reshaped_output)\n",
    "\n",
    "            # Each 'sdp_output' is a Tensor of shape (N, seq_len, embed_len/num_heads) -> (8, 20, 64).\n",
    "            # Each flattened Tensor has length (8, 20*64) = 10240\n",
    "            #print('sdp_output shape: ', sdp_output.shape)\n",
    "            #print('sdp_output flattened length: ', torch.flatten(sdp_output, start_dim=1, end_dim=2).shape)\n",
    "            \n",
    "            # We need to concatenate the outputs of all the heads\n",
    "            # into one vector and pass it through a final linear layer\n",
    "            self.concat_output.append(torch.flatten(sdp_output, start_dim=1, end_dim=2))\n",
    "            \n",
    "        flattened_concat_output = torch.flatten(torch.stack(self.concat_output), start_dim=1, end_dim=2)\n",
    "        \n",
    "        # Pass the concatenated vector in a final linear layer and return output\n",
    "        return self.output_linear(flattened_concat_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead = MultiHeadAttention(mask=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = multihead.forward(test_input, test_input, test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_output.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Encoder block. I will then stack this into multiple layers to create the Encoder stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len, dropout=dropout):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.embed_len = embed_len\n",
    "        self.dropout = dropout\n",
    "        self.multihead = MultiHeadAttention()             # Multi-Head Attention layer\n",
    "        self.firstNorm = nn.LayerNorm(embed_len)          # Normalization layer (after the multi-head attention layer)\n",
    "        self.secondNorm = nn.LayerNorm(embed_len)         # Normalization layer (after the Feed Forward layer)\n",
    "        self.dropoutLayer = nn.Dropout(p=self.dropout)    # Dropout layer (before addition and normalization)\n",
    "\n",
    "        # The Feed Forward layer. In the paper this has input &\n",
    "        # output = 512 (or = embed_len) and inner-layer = 2048 (or = embed_len*4)\n",
    "        self.feedForward = nn.Sequential(\n",
    "            nn.Linear(embed_len, embed_len*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_len*4, embed_len)\n",
    "        )\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        attention_output = self.multihead.forward(queries, keys, values)\n",
    "        attention_output = self.dropoutLayer(attention_output)\n",
    "\n",
    "        # the output of the first residual connection\n",
    "        first_sublayer_output = self.firstNorm(attention_output + queries)\n",
    "\n",
    "        ff_output = self.feedForward(first_sublayer_output)\n",
    "        ff_output = self.dropoutLayer(ff_output)\n",
    "\n",
    "        # return the output of the second residual connection\n",
    "        return self.secondNorm(ff_output + first_sublayer_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Decoder block. I will also stack this into multiple layers to create the Decoder stack.\n",
    "\n",
    "The decoder has a total of 3 inputs: the queries, which come from the previous decoder layer, and the memory keys and values, which come from the output of the encoder (Section 3.2.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len, dropout=dropout):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.embed_len = embed_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Masked Multi-Head Attention and Normalization layers.\n",
    "        self.maskedMultihead = MultiHeadAttention(mask=True)\n",
    "        self.firstNorm = nn.LayerNorm(self.embed_len)\n",
    "\n",
    "        self.dropoutLayer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        # The output of the above two layers and the output from the encoder stack feed \n",
    "        # into an 'encoder block'\n",
    "        self.encoderBlock = EncoderBlock()\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "\n",
    "        # First sublayer, which consists of the Masked Multi-Head Attention + Normalization\n",
    "        # sublayer, with a residual connection\n",
    "        masked_multihead_output = self.maskedMultihead.forward(queries, queries, queries)\n",
    "        masked_multihead_output = self.dropoutLayer(masked_multihead_output)\n",
    "        first_sublayer_output = self.firstNorm(masked_multihead_output + queries)\n",
    "\n",
    "        # The remaining of the DecoderBlock is basically an encoder block, which takes keys \n",
    "        # and values from the actual Encoder stack output, and takes queries from the \n",
    "        # previous sublayer of the DecoderBlock\n",
    "        return self.encoderBlock.forward(first_sublayer_output, keys, values)      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create whole Transformer block (still missing some components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, stack_len=stack_len, embed_len=embed_len, output_vocab_size=output_vocab_size):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.stack_len = stack_len\n",
    "        self.embed_len = embed_len\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "\n",
    "        self.encStack = nn.ModuleList([EncoderBlock() for i in range(self.stack_len)])\n",
    "        self.decStack = nn.ModuleList([DecoderBlock(mask=True) for i in range(self.stack_len)])\n",
    "        self.finalLinear = nn.Linear(self.embed_len, self.output_vocab_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, test_input):\n",
    "        enc_output = test_input\n",
    "\n",
    "        # Final output 'enc_output' of this loop will be both the key and value\n",
    "        # that will be taken as input to the second sub-layer of the decoder\n",
    "        for enc_layer in self.encStack:\n",
    "            enc_output = enc_layer.forward(enc_output, enc_output, enc_output)\n",
    "\n",
    "        # Decoder stack will take the 'enc_output' from the decoder as the keys\n",
    "        # and values, and will take its own output from the previous layer as\n",
    "        # the query. The query used for the first layer is the '<sos>' token.\n",
    "        for dec_layer in self.decStack:\n",
    "            dec_output = dec_layer.forward(dec_output, enc_output, enc_output)\n",
    "\n",
    "        # Pass the final decoder stack output to the linear layer that takes in\n",
    "        # input vector of size 'embed_len' and outputs a vector that has the \n",
    "        # size of the vocab specified. Finall return the softmax output of that vector\n",
    "        final_output = self.finalLinear(dec_output)\n",
    "\n",
    "        return self.softmax(final_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe19fb5bed516e6ac74a9b255f69d3cfb16eeb4b5a81b7a48fdeb18da73b6471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
