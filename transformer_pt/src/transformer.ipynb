{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-Do:\n",
    "- Import WMT dataset and perform actual training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achalhoub/miniconda3/envs/transformer_pt/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify device as GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a single batch of random input just to use to check if the shape initializations in the below classes are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters. I used the same ones as those used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "embed_len = 512\n",
    "seq_len = 20                # just a dummy number for now\n",
    "batch_size = 8              # just a dummy number for now\n",
    "stack_len = 6               # length of encoder and decoder stacks (=6 as used in paper)\n",
    "dropout = 0.1               # dropout value to use\n",
    "\n",
    "output_vocab_size = 37000    # just a dummy number for now\n",
    "input_vocab_size = 37000    # just a dummy number for now (from paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "test_input = torch.rand((batch_size, seq_len, embed_len)).to(device)\n",
    "print(test_input.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Input embedding class. This includes both the normal embedding for the tokenized input sequences, as well as the positional embeddings. They are added together and the sum of them is used as the input embedding to the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, input_vocab_size=input_vocab_size, embed_len=embed_len, dropout=dropout, device=device):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.embed_len = embed_len\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.firstEmbedding = nn.Embedding(self.input_vocab_size, self.embed_len)\n",
    "        self.secondEmbedding = nn.Embedding(self.input_vocab_size, self.embed_len)\n",
    "\n",
    "        self.dropoutLayer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        first_embedding = self.firstEmbedding(input)\n",
    "        \n",
    "        batch_size, seq_len = input.shape\n",
    "\n",
    "        positions_vector = torch.arange(0, seq_len).expand(batch_size, seq_len).to(self.device)\n",
    "        second_embedding = self.secondEmbedding(positions_vector)\n",
    "\n",
    "        return self.dropoutLayer(first_embedding + second_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 20])\n",
      "torch.Size([8, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "embedding = InputEmbedding().to(device)\n",
    "before_embedding = torch.randint(10, (8, 20)).to(device)\n",
    "after_embedding = embedding.forward(before_embedding)\n",
    "print(before_embedding.shape)\n",
    "print(after_embedding.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin building the Transformer. The first step is to build the 'Scaled Dot-Product Attention' block mentioned in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProduct(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len, mask=None):\n",
    "        super(ScaledDotProduct, self).__init__()\n",
    "        \n",
    "        self.dk = embed_len                 # dk = embed_len\n",
    "        self.mask = mask\n",
    "        self.softmax = nn.Softmax(dim=3)    # Softmax operator\n",
    "\n",
    "    # Define the forward function\n",
    "    def forward(self, queries, keys, values):       \n",
    "\n",
    "        # First batch MatMul operation & scaling down by sqrt(dk).\n",
    "        # Output 'compatibility' has shape:\n",
    "        # (batch_size, num_heads, seq_len, seq_len) -> (8, 8, 20, 20).\n",
    "        compatibility = torch.matmul(queries, torch.transpose(keys, 2, 3)) \n",
    "        compatibility = compatibility / math.sqrt((self.dk))               \n",
    "\n",
    "        # Apply mask after scaling the result of MatMul of Q and K.\n",
    "        # This is needed in the decoder to prevent the decoder from\n",
    "        # 'peaking ahead' and knowing what word will come next.\n",
    "        # Check: https://pytorch.org/docs/stable/generated/torch.tril.html \n",
    "        if self.mask is not None:\n",
    "            compatibility = torch.tril(compatibility)\n",
    "            \n",
    "        # Normalize using Softmax\n",
    "        compatibility_softmax = self.softmax(compatibility)        \n",
    "               \n",
    "        return torch.matmul(compatibility_softmax, torch.transpose(values, 1, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the 'Multi-Head Attention' block. Init variable need simplifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads=num_heads, embed_len=embed_len, batch_size=batch_size, seq_len=seq_len, mask=None):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_len = embed_len\n",
    "        self.head_length = int(self.embed_len/self.num_heads)\n",
    "        self.mask = mask\n",
    "        self.concat_output = []\n",
    "\n",
    "        # Q, K, and V have shape: (batch_size, seq_len, embed_len)\n",
    "        self.q_in = self.k_in = self.v_in = self.embed_len\n",
    "\n",
    "        # Linear layers take in embed_len as input \n",
    "        # dim and produce embed_len as output dim\n",
    "        self.q_linear = nn.Linear(int(self.q_in), int(self.q_in))\n",
    "        self.k_linear = nn.Linear(int(self.k_in), int(self.k_in))\n",
    "        self.v_linear = nn.Linear(int(self.v_in), int(self.v_in))\n",
    "\n",
    "        # Attention layer.\n",
    "        if self.mask is not None:\n",
    "            self.attention = ScaledDotProduct(mask=True) \n",
    "        else:\n",
    "            self.attention = ScaledDotProduct()\n",
    "\n",
    "        self.output_linear = nn.Linear(self.q_in, self.embed_len)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "\n",
    "        # Query has shape: (batch_size, seq_len, num_heads, head_length) -> (8, 20, 8, 64).\n",
    "        # Then transpose it: (batch_size, num_heads, seq_len, head_length) -> (8, 8, 20, 64).\n",
    "        queries = self.q_linear(queries).reshape(\n",
    "            self.batch_size, self.seq_len, self.num_heads, self.head_length)\n",
    "        queries = queries.transpose(1, 2)\n",
    "\n",
    "        # Same for Key as for Query above.\n",
    "        keys = self.k_linear(keys).reshape(\n",
    "            self.batch_size, self.seq_len, self.num_heads, self.head_length)\n",
    "        keys = keys.transpose(1, 2)\n",
    "\n",
    "        # Value has shape: (batch_size, seq_len, num_heads, head_length) -> (8, 20, 8, 64)\n",
    "        values = self.v_linear(values).reshape(\n",
    "            self.batch_size, self.seq_len, self.num_heads, self.head_length)\n",
    "\n",
    "        # 'sdp_output' here has size: \n",
    "        # (batch_size, num_heads, seq_len, head_length) -> (8, 8, 20, 64)\n",
    "        sdp_output = self.attention.forward(queries, keys, values)\n",
    "\n",
    "        # Reshape to (batch_size, seq_len, num_heads*head_length) -> (8, 20, 512)\n",
    "        sdp_output = sdp_output.transpose(1, 2).reshape(\n",
    "            self.batch_size, self.seq_len, self.num_heads * self.head_length)\n",
    "\n",
    "        # Return self.output_linear(sdp_output).\n",
    "        # This has shape (batch_size, seq_len, embed_len) -> (8, 20, 512)\n",
    "        return self.output_linear(sdp_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead = MultiHeadAttention().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "test_output = multihead.forward(after_embedding, after_embedding, after_embedding)\n",
    "print(test_output.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Encoder block. I will then stack this into multiple layers to create the Encoder stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len, dropout=dropout):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.embed_len = embed_len\n",
    "        self.dropout = dropout\n",
    "        self.multihead = MultiHeadAttention()             # Multi-Head Attention layer\n",
    "        self.firstNorm = nn.LayerNorm(embed_len)          # Normalization layer (after the multi-head attention layer)\n",
    "        self.secondNorm = nn.LayerNorm(embed_len)         # Normalization layer (after the Feed Forward layer)\n",
    "        self.dropoutLayer = nn.Dropout(p=self.dropout)    # Dropout layer (before addition and normalization)\n",
    "\n",
    "        # The Feed Forward layer. In the paper this has input &\n",
    "        # output = 512 (or = embed_len) and inner-layer = 2048 (or = embed_len*4)\n",
    "        self.feedForward = nn.Sequential(\n",
    "            nn.Linear(embed_len, embed_len*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_len*4, embed_len)\n",
    "        )\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        attention_output = self.multihead.forward(queries, keys, values)\n",
    "        attention_output = self.dropoutLayer(attention_output)\n",
    "        print('attention_output shape: ', attention_output.size())\n",
    "        print('queries shape: ', queries.size())\n",
    "\n",
    "        # the output of the first residual connection\n",
    "        first_sublayer_output = self.firstNorm(attention_output + queries)\n",
    "\n",
    "        ff_output = self.feedForward(first_sublayer_output)\n",
    "        ff_output = self.dropoutLayer(ff_output)\n",
    "\n",
    "        # return the output of the second residual connection\n",
    "        return self.secondNorm(ff_output + first_sublayer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_output shape:  torch.Size([8, 20, 512])\n",
      "queries shape:  torch.Size([8, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderBlock().to(device)\n",
    "test_encoder = encoder.forward(after_embedding, after_embedding, after_embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Decoder block. I will also stack this into multiple layers to create the Decoder stack.\n",
    "\n",
    "The decoder has a total of 3 inputs: the queries, which come from the previous decoder layer, and the memory keys and values, which come from the output of the encoder (Section 3.2.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len, dropout=dropout):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.embed_len = embed_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Masked Multi-Head Attention and Normalization layers.\n",
    "        self.maskedMultihead = MultiHeadAttention(mask=True)\n",
    "        self.firstNorm = nn.LayerNorm(self.embed_len)\n",
    "\n",
    "        self.dropoutLayer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        # The output of the above two layers and the output from the encoder stack feed \n",
    "        # into an 'encoder block'\n",
    "        self.encoderBlock = EncoderBlock()\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "\n",
    "        # First sublayer, which consists of the Masked Multi-Head Attention + Normalization\n",
    "        # sublayer, with a residual connection\n",
    "        masked_multihead_output = self.maskedMultihead.forward(queries, queries, queries)\n",
    "        masked_multihead_output = self.dropoutLayer(masked_multihead_output)\n",
    "        first_sublayer_output = self.firstNorm(masked_multihead_output + queries)\n",
    "\n",
    "        # The remaining of the DecoderBlock is basically an encoder block, which takes keys \n",
    "        # and values from the actual Encoder stack output, and takes queries from the \n",
    "        # previous sublayer of the DecoderBlock\n",
    "        return self.encoderBlock.forward(first_sublayer_output, keys, values)      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create whole Transformer block (still missing some components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, stack_len=stack_len, embed_len=embed_len, device=device, output_vocab_size=output_vocab_size):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.stack_len = stack_len\n",
    "        self.embed_len = embed_len\n",
    "        self.device = device\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "\n",
    "        self.encStack = nn.ModuleList([EncoderBlock() for i in range(self.stack_len)])\n",
    "        self.decStack = nn.ModuleList([DecoderBlock() for i in range(self.stack_len)])\n",
    "        self.finalLinear = nn.Linear(self.embed_len, self.output_vocab_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, test_input):\n",
    "\n",
    "        embedding = InputEmbedding().to(self.device)\n",
    "        print('test_input type: ', type(test_input))\n",
    "        enc_output = embedding.forward(test_input.to(self.device))\n",
    "        #enc_output = test_input\n",
    "        print('initial enc_out size: ', enc_output.size())\n",
    "\n",
    "        # Final output 'enc_output' of this loop will be both the key and value\n",
    "        # that will be taken as input to the second sub-layer of the decoder\n",
    "        for enc_layer in self.encStack:\n",
    "            enc_output = enc_layer.forward(enc_output, enc_output, enc_output)\n",
    "            print('second enc_output size: ', enc_output.size())\n",
    "\n",
    "        # Decoder stack will take the 'enc_output' from the decoder as the keys\n",
    "        # and values, and will take its own output from the previous layer as\n",
    "        # the query. The query used for the first layer is the '<sos>' token.\n",
    "        for dec_layer in self.decStack:\n",
    "            dec_output = dec_layer.forward(dec_output, enc_output, enc_output)\n",
    "\n",
    "        # Pass the final decoder stack output to the linear layer that takes in\n",
    "        # input vector of size 'embed_len' and outputs a vector that has the \n",
    "        # size of the vocab specified. Finall return the softmax output of that vector\n",
    "        final_output = self.finalLinear(dec_output)\n",
    "\n",
    "        return self.softmax(final_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with random example input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = torch.randint(10, (8, 20)).to(device)\n",
    "Embedding = InputEmbedding().to(device)\n",
    "input_embeddings = Embedding.forward(input_tokens)\n",
    "input_embeddings = input_embeddings.to(device)\n",
    "transformer = Transformer().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(input_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_input type:  <class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transformer_output \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mforward(input_embeddings)\n",
      "Cell \u001b[0;32mIn[91], line 18\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, test_input)\u001b[0m\n\u001b[1;32m     16\u001b[0m embedding \u001b[39m=\u001b[39m InputEmbedding()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtest_input type: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mtype\u001b[39m(test_input))\n\u001b[0;32m---> 18\u001b[0m enc_output \u001b[39m=\u001b[39m embedding\u001b[39m.\u001b[39;49mforward(test_input\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice))\n\u001b[1;32m     19\u001b[0m \u001b[39m#enc_output = test_input\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minitial enc_out size: \u001b[39m\u001b[39m'\u001b[39m, enc_output\u001b[39m.\u001b[39msize())\n",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m, in \u001b[0;36mInputEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> 15\u001b[0m     first_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfirstEmbedding(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     17\u001b[0m     batch_size, seq_len \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape\n\u001b[1;32m     19\u001b[0m     positions_vector \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, seq_len)\u001b[39m.\u001b[39mexpand(batch_size, seq_len)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/transformer_pt/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/transformer_pt/lib/python3.10/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/miniconda3/envs/transformer_pt/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "transformer_output = transformer.forward(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe19fb5bed516e6ac74a9b255f69d3cfb16eeb4b5a81b7a48fdeb18da73b6471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
