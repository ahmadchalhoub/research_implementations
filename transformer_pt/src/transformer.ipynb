{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-Do:\n",
    "- Import WMT dataset and perform actual training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achalhoub/miniconda3/envs/transformer_pt/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify device as GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a single batch of random input just to use to check if the shape initializations in the below classes are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters. I used the same ones as those used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "embed_len = 512\n",
    "seq_len = 20                # just a dummy number for now\n",
    "batch_size = 8              # just a dummy number for now\n",
    "stack_len = 6               # length of encoder and decoder stacks (=6 as used in paper)\n",
    "dropout = 0.1               # dropout value to use\n",
    "\n",
    "output_vocab_size = 37000    # just a dummy number for now\n",
    "input_vocab_size = 37000    # just a dummy number for now (from paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "test_input = torch.rand((batch_size, seq_len, embed_len)).to(device)\n",
    "print(test_input.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Input embedding class. This includes both the normal embedding for the tokenized input sequences, as well as the positional embeddings. They are added together and the sum of them is used as the input embedding to the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, input_vocab_size=input_vocab_size, embed_len=embed_len, dropout=dropout, device=device):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.embed_len = embed_len\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.firstEmbedding = nn.Embedding(self.input_vocab_size, self.embed_len)\n",
    "        self.secondEmbedding = nn.Embedding(self.input_vocab_size, self.embed_len)\n",
    "\n",
    "        self.dropoutLayer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        first_embedding = self.firstEmbedding(input)\n",
    "        \n",
    "        batch_size, seq_len = input.shape\n",
    "\n",
    "        positions_vector = torch.arange(0, seq_len).expand(batch_size, seq_len).to(self.device)\n",
    "        second_embedding = self.secondEmbedding(positions_vector)\n",
    "\n",
    "        return self.dropoutLayer(first_embedding + second_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before_embedding type:  <class 'torch.Tensor'>\n",
      "torch.Size([8, 20])\n",
      "torch.Size([8, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "embedding = InputEmbedding().to(device)\n",
    "before_embedding = torch.randint(10, (8, 20)).to(device)\n",
    "print('before_embedding type: ', type(before_embedding))\n",
    "after_embedding = embedding.forward(before_embedding)\n",
    "print(before_embedding.shape)\n",
    "print(after_embedding.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin building the Transformer. The first step is to build the 'Scaled Dot-Product Attention' block mentioned in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProduct(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len, mask=None):\n",
    "        super(ScaledDotProduct, self).__init__()\n",
    "        \n",
    "        self.dk = embed_len                 # dk = embed_len\n",
    "        self.mask = mask\n",
    "        self.softmax = nn.Softmax(dim=3)    # Softmax operator\n",
    "\n",
    "    # Define the forward function\n",
    "    def forward(self, queries, keys, values):       \n",
    "\n",
    "        # First batch MatMul operation & scaling down by sqrt(dk).\n",
    "        # Output 'compatibility' has shape:\n",
    "        # (batch_size, num_heads, seq_len, seq_len) -> (8, 8, 20, 20).\n",
    "        compatibility = torch.matmul(queries, torch.transpose(keys, 2, 3)) \n",
    "        compatibility = compatibility / math.sqrt((self.dk))               \n",
    "\n",
    "        # Apply mask after scaling the result of MatMul of Q and K.\n",
    "        # This is needed in the decoder to prevent the decoder from\n",
    "        # 'peaking ahead' and knowing what word will come next.\n",
    "        # Check: https://pytorch.org/docs/stable/generated/torch.tril.html \n",
    "        if self.mask is not None:\n",
    "            compatibility = torch.tril(compatibility)\n",
    "            \n",
    "        # Normalize using Softmax\n",
    "        compatibility_softmax = self.softmax(compatibility)        \n",
    "               \n",
    "        return torch.matmul(compatibility_softmax, torch.transpose(values, 1, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the 'Multi-Head Attention' block. Init variable need simplifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads=num_heads, embed_len=embed_len, batch_size=batch_size, seq_len=seq_len, mask=None):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_len = embed_len\n",
    "        self.head_length = int(self.embed_len/self.num_heads)\n",
    "        self.mask = mask\n",
    "        self.concat_output = []\n",
    "\n",
    "        # Q, K, and V have shape: (batch_size, seq_len, embed_len)\n",
    "        self.q_in = self.k_in = self.v_in = self.embed_len\n",
    "\n",
    "        # Linear layers take in embed_len as input \n",
    "        # dim and produce embed_len as output dim\n",
    "        self.q_linear = nn.Linear(int(self.q_in), int(self.q_in))\n",
    "        self.k_linear = nn.Linear(int(self.k_in), int(self.k_in))\n",
    "        self.v_linear = nn.Linear(int(self.v_in), int(self.v_in))\n",
    "\n",
    "        # Attention layer.\n",
    "        if self.mask is not None:\n",
    "            self.attention = ScaledDotProduct(mask=True) \n",
    "        else:\n",
    "            self.attention = ScaledDotProduct()\n",
    "\n",
    "        self.output_linear = nn.Linear(self.q_in, self.embed_len)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "\n",
    "        # Query has shape: (batch_size, seq_len, num_heads, head_length) -> (8, 20, 8, 64).\n",
    "        # Then transpose it: (batch_size, num_heads, seq_len, head_length) -> (8, 8, 20, 64).\n",
    "        queries = self.q_linear(queries).reshape(\n",
    "            self.batch_size, self.seq_len, self.num_heads, self.head_length)\n",
    "        queries = queries.transpose(1, 2)\n",
    "\n",
    "        # Same for Key as for Query above.\n",
    "        keys = self.k_linear(keys).reshape(\n",
    "            self.batch_size, self.seq_len, self.num_heads, self.head_length)\n",
    "        keys = keys.transpose(1, 2)\n",
    "\n",
    "        # Value has shape: (batch_size, seq_len, num_heads, head_length) -> (8, 20, 8, 64)\n",
    "        values = self.v_linear(values).reshape(\n",
    "            self.batch_size, self.seq_len, self.num_heads, self.head_length)\n",
    "\n",
    "        # 'sdp_output' here has size: \n",
    "        # (batch_size, num_heads, seq_len, head_length) -> (8, 8, 20, 64)\n",
    "        sdp_output = self.attention.forward(queries, keys, values)\n",
    "\n",
    "        # Reshape to (batch_size, seq_len, num_heads*head_length) -> (8, 20, 512)\n",
    "        sdp_output = sdp_output.transpose(1, 2).reshape(\n",
    "            self.batch_size, self.seq_len, self.num_heads * self.head_length)\n",
    "\n",
    "        # Return self.output_linear(sdp_output).\n",
    "        # This has shape (batch_size, seq_len, embed_len) -> (8, 20, 512)\n",
    "        return self.output_linear(sdp_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead = MultiHeadAttention().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "test_output = multihead.forward(after_embedding, after_embedding, after_embedding)\n",
    "print(test_output.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Encoder block. I will then stack this into multiple layers to create the Encoder stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len, dropout=dropout):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.embed_len = embed_len\n",
    "        self.dropout = dropout\n",
    "        self.multihead = MultiHeadAttention()             # Multi-Head Attention layer\n",
    "        self.firstNorm = nn.LayerNorm(embed_len)          # Normalization layer (after the multi-head attention layer)\n",
    "        self.secondNorm = nn.LayerNorm(embed_len)         # Normalization layer (after the Feed Forward layer)\n",
    "        self.dropoutLayer = nn.Dropout(p=self.dropout)    # Dropout layer (before addition and normalization)\n",
    "\n",
    "        # The Feed Forward layer. In the paper this has input &\n",
    "        # output = 512 (or = embed_len) and inner-layer = 2048 (or = embed_len*4)\n",
    "        self.feedForward = nn.Sequential(\n",
    "            nn.Linear(embed_len, embed_len*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_len*4, embed_len)\n",
    "        )\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        attention_output = self.multihead.forward(queries, keys, values)\n",
    "        attention_output = self.dropoutLayer(attention_output)\n",
    "\n",
    "        # the output of the first residual connection\n",
    "        first_sublayer_output = self.firstNorm(attention_output + queries)\n",
    "\n",
    "        ff_output = self.feedForward(first_sublayer_output)\n",
    "        ff_output = self.dropoutLayer(ff_output)\n",
    "\n",
    "        # return the output of the second residual connection\n",
    "        return self.secondNorm(ff_output + first_sublayer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderBlock().to(device)\n",
    "test_encoder = encoder.forward(after_embedding, after_embedding, after_embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Decoder block. I will also stack this into multiple layers to create the Decoder stack.\n",
    "\n",
    "The decoder has a total of 3 inputs: the queries, which come from the previous decoder layer, and the memory keys and values, which come from the output of the encoder (Section 3.2.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len, dropout=dropout):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.embed_len = embed_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Masked Multi-Head Attention and Normalization layers.\n",
    "        self.maskedMultihead = MultiHeadAttention(mask=True)\n",
    "        self.firstNorm = nn.LayerNorm(self.embed_len)\n",
    "\n",
    "        self.dropoutLayer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        # The output of the above two layers and the output from the encoder stack feed \n",
    "        # into an 'encoder block'\n",
    "        self.encoderBlock = EncoderBlock()\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "\n",
    "        # First sublayer, which consists of the Masked Multi-Head Attention + Normalization\n",
    "        # sublayer, with a residual connection\n",
    "        masked_multihead_output = self.maskedMultihead.forward(queries, queries, queries)\n",
    "        masked_multihead_output = self.dropoutLayer(masked_multihead_output)\n",
    "        first_sublayer_output = self.firstNorm(masked_multihead_output + queries)\n",
    "\n",
    "        # The remaining of the DecoderBlock is basically an encoder block, which takes keys \n",
    "        # and values from the actual Encoder stack output, and takes queries from the \n",
    "        # previous sublayer of the DecoderBlock\n",
    "        return self.encoderBlock.forward(first_sublayer_output, keys, values)      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create whole Transformer block (still missing some components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, stack_len=stack_len, embed_len=embed_len, device=device, output_vocab_size=output_vocab_size):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.stack_len = stack_len\n",
    "        self.embed_len = embed_len\n",
    "        self.device = device\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "\n",
    "        self.embedding = InputEmbedding().to(self.device)\n",
    "        self.encStack = nn.ModuleList([EncoderBlock() for i in range(self.stack_len)])\n",
    "        self.decStack = nn.ModuleList([DecoderBlock() for i in range(self.stack_len)])\n",
    "        self.finalLinear = nn.Linear(self.embed_len, self.output_vocab_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, test_input, test_target):\n",
    "\n",
    "        enc_output = self.embedding.forward(test_input)\n",
    "\n",
    "        # Final output 'enc_output' of this loop will be both the key and value\n",
    "        # that will be taken as input to the second sub-layer of the decoder\n",
    "        for enc_layer in self.encStack:\n",
    "            enc_output = enc_layer.forward(enc_output, enc_output, enc_output)\n",
    "\n",
    "        # Decoder stack will take the 'enc_output' from the decoder as the keys\n",
    "        # and values, and will take its own output from the previous layer as\n",
    "        # the query. The query used for the first layer is the '<sos>' token.\n",
    "        dec_output = self.embedding(test_target)\n",
    "        for dec_layer in self.decStack:\n",
    "            dec_output = dec_layer.forward(dec_output, enc_output, enc_output)\n",
    "\n",
    "        # Pass the final decoder stack output to the linear layer that takes in\n",
    "        # input vector of size 'embed_len' and outputs a vector that has the \n",
    "        # size of the vocab specified. Finall return the softmax output of that vector\n",
    "        final_output = self.finalLinear(dec_output)\n",
    "\n",
    "        return self.softmax(final_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with random example input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([8, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "input_tokens = torch.randint(10, (batch_size, seq_len)).to(device)\n",
    "output_target = torch.randint(10, (batch_size, seq_len)).to(device)\n",
    "Embedding = InputEmbedding().to(device)\n",
    "input_embeddings = Embedding.forward(input_tokens)\n",
    "input_embeddings = input_embeddings.to(device)\n",
    "transformer = Transformer().to(device)\n",
    "print(type(input_tokens))\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11296/475266861.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.softmax(final_output)\n"
     ]
    }
   ],
   "source": [
    "transformer_output = transformer.forward(input_tokens, output_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 20, 37000])\n"
     ]
    }
   ],
   "source": [
    "print(transformer_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe19fb5bed516e6ac74a9b255f69d3cfb16eeb4b5a81b7a48fdeb18da73b6471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
