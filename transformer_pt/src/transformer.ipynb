{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load English and German tokenizers using Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_lang = spacy.load(\"en_core_web_sm\")\n",
    "ger_lang = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify device as GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a single batch of random input just to use to check if the shape initializations in the below classes are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test input has shape (N, seq_len, embed_len) => ((8, 20, 512)).\n",
    "# I decreased the batch size from 64 to 8 because of memory issues of my GPU.\n",
    "# 64 was too big for the memory, and the biggest size that worked was 8.\n",
    "\n",
    "test_query = torch.rand((8, 20, 512)).to(device)\n",
    "test_key = torch.rand((8, 20, 512)).to(device)\n",
    "test_value = torch.rand((8, 20, 512)).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters. I used the same ones as those used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "d_model = 512"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin building the Transformer. The first step is to build the 'Scaled Dot-Product Attention' block mentioned in the paper. This is still just the first draft; it will probably need some fixes once I get to later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProduct(nn.Module):\n",
    "    def __init__(self, queries):\n",
    "        super(ScaledDotProduct, self).__init__()\n",
    "        \n",
    "        # I probably don't need to initialize K, Q, and V here, since they will be passed to the \n",
    "        # scaled dot product when we call it from the MultiHeadAttention class in the forward method.\n",
    "        # Will delete the variable below later if I turn out to be right.\n",
    "        self.queries = queries\n",
    "        #self.keys = keys\n",
    "        #self.values = values\n",
    "        self.dk = self.queries.shape[1]\n",
    "\n",
    "        # Softmax operator. 'dim' still needs to be specified\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    # Define the forward function\n",
    "    def forward(self, queries, keys, values):\n",
    "        compatibility = torch.bmm(queries, torch.transpose(keys))   # first batch MatMul operation\n",
    "        compatibility = compatibility / torch.sqrt((self.dk))       # scaling down by sqrt(dk)\n",
    "        compatibility_softmax = self.softmax(compatibility)         # normalizing using Softmax\n",
    "        output = torch.bmm(compatibility_softmax, values)           # final batch MatMul operation\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the 'Multi-Head Attention' block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, queries, keys, values):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = h\n",
    "        self.batch_num = queries.shape[0]\n",
    "        self.seq_len = queries.shape[1]\n",
    "        self.embed_len = queries.shape[2]\n",
    "        self.d_model = d_model\n",
    "        self.queries = queries\n",
    "        self.keys = keys\n",
    "        self.values = values\n",
    "        self.head_length = self.d_model/self.num_heads\n",
    "\n",
    "        self.concat_output = []\n",
    "\n",
    "        # For an input, Q for example, which would originally have a shape\n",
    "        # of (N, seq_len, embed_len), it would be split up into the number of \n",
    "        # heads that we define (ex: 8). So, the new shape would be\n",
    "        # (N, seq_len, embed_len/8). This would also apply to K and V too.\n",
    "\n",
    "        # Since we are flattening batches of matrices, I'm not sure if the flattening\n",
    "        # should be done in another way. I'll come back to this later if it needs changing.\n",
    "        self.q_in = int(torch.flatten(self.queries).size(dim=0) / self.num_heads)\n",
    "        self.k_in = int(torch.flatten(self.keys).size(dim=0) / self.num_heads)\n",
    "        self.v_in = int(torch.flatten(self.values).size(dim=0) / self.num_heads)\n",
    "        \n",
    "        # For the input of each Linear layer, we would have the divided Q, K, \n",
    "        # and V calculated above. \n",
    "        self.q_linear = nn.Linear(self.q_in, self.q_in)\n",
    "        self.k_linear = nn.Linear(self.k_in, self.k_in)\n",
    "        self.v_linear = nn.Linear(self.v_in, self.v_in)\n",
    "\n",
    "        # Attention layer.\n",
    "        self.attention = ScaledDotProduct(self.queries)\n",
    "\n",
    "        # This is the final Linear layer, after the outputs of all the heads\n",
    "        # from the Scaled Dot Product layer have been concatenated together. The\n",
    "        # output dimension of this layer is a hyperparameter that we define. Here\n",
    "        # we use d_model, which is 512.\n",
    "        self.output_linear = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # Feed the 8 heads of Q, K, and V into the linear layers in parallel, and then into the\n",
    "        # attention block. Let's say the original tensor Q has the following shape: \n",
    "        # (N, seq_len, embed_len) -> (64, 20, 512).\n",
    "        # The segment that will go into each head will be of the following size:\n",
    "        # (N, seq_len, embed_len/num_heads) -> (64, 20, 64). So we need to slice the third dimension.\n",
    "        for i in range(self.num_heads):\n",
    "\n",
    "            # The output of each of the linear layers has length -> (N*seq_len*embed_len/num_heads)\n",
    "            q_linear_output = self.q_linear(torch.flatten(queries[:, :, int(i*self.head_length):int((i+1)*self.head_length)]))\n",
    "            k_linear_output = self.k_linear(torch.flatten(keys[:, :, int(i*self.head_length):int((i+1)*self.head_length)]))\n",
    "            v_linear_output = self.v_linear(torch.flatten(values[:, :, int(i*self.head_length):int((i+1)*self.head_length)]))\n",
    "\n",
    "            # Since the three outputs computed from the linear layers above are just 1D vectors of length\n",
    "            # (N*seq_len*embed_len/num_heads), and the ScaledDotProduct forward method expects 3D tensors,\n",
    "            # I will reshape the 1D vectors into 3D tensors of shape (N, seq_len, embed_len/num_heads)\n",
    "            q_reshaped_output = torch.reshape(q_linear_output, (self.batch_num, self.seq_len, int(self.embed_len/self.num_heads)))\n",
    "            k_reshaped_output = torch.reshape(k_linear_output, (self.batch_num, self.seq_len, int(self.embed_len/self.num_heads)))\n",
    "            v_reshaped_output = torch.reshape(v_linear_output, (self.batch_num, self.seq_len, int(self.embed_len/self.num_heads)))\n",
    "\n",
    "            # Feed reshaped Q, K, and V into ScaledDotProduct layer.\n",
    "            # 'sdp_output' should have shape (N, seq_len, embed_len/num_heads)\n",
    "            sdp_output = self.attention.forward(q_reshaped_output, k_reshaped_output, v_reshaped_output)\n",
    "\n",
    "            # We need to concatenate the outputs of all the heads\n",
    "            # into one vector and pass it through a final linear layer\n",
    "            concat_output = self.concat_output.append(torch.flatten(sdp_output))\n",
    "\n",
    "        print('concat_output shape: ', concat_output.shape)\n",
    "        # Pass the concatenated vector in a final linear layer\n",
    "        #return self.output_linear(self.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead = MultiHeadAttention(num_heads, d_model, test_query, test_key, test_value).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transpose() received an invalid combination of arguments - got (Tensor), but expected one of:\n * (Tensor input, int dim0, int dim1)\n * (Tensor input, name dim0, name dim1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_output \u001b[39m=\u001b[39m multihead\u001b[39m.\u001b[39;49mforward(test_query, test_key, test_value)\n",
      "Cell \u001b[0;32mIn[26], line 64\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, queries, keys, values)\u001b[0m\n\u001b[1;32m     60\u001b[0m v_reshaped_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mreshape(v_linear_output, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_num, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_len, \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_len\u001b[39m/\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads)))\n\u001b[1;32m     62\u001b[0m \u001b[39m# Feed reshaped Q, K, and V into ScaledDotProduct layer.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m# 'sdp_output' should have shape (N, seq_len, embed_len/num_heads)\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m sdp_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention\u001b[39m.\u001b[39;49mforward(q_reshaped_output, k_reshaped_output, v_reshaped_output)\n\u001b[1;32m     66\u001b[0m \u001b[39m# We need to concatenate the outputs of all the heads\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m# into one vector and pass it through a final linear layer\u001b[39;00m\n\u001b[1;32m     68\u001b[0m concat_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconcat_output\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mflatten(sdp_output))\n",
      "Cell \u001b[0;32mIn[25], line 18\u001b[0m, in \u001b[0;36mScaledDotProduct.forward\u001b[0;34m(self, queries, keys, values)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, queries, keys, values):\n\u001b[0;32m---> 18\u001b[0m     compatibility \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(queries, torch\u001b[39m.\u001b[39;49mtranspose(keys))   \u001b[39m# first batch MatMul operation\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     compatibility \u001b[39m=\u001b[39m compatibility \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39msqrt((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdk))       \u001b[39m# scaling down by sqrt(dk)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     compatibility_softmax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax(compatibility)         \u001b[39m# normalizing using Softmax\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: transpose() received an invalid combination of arguments - got (Tensor), but expected one of:\n * (Tensor input, int dim0, int dim1)\n * (Tensor input, name dim0, name dim1)\n"
     ]
    }
   ],
   "source": [
    "test_output = multihead.forward(test_query, test_key, test_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe19fb5bed516e6ac74a9b255f69d3cfb16eeb4b5a81b7a48fdeb18da73b6471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
