{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer implementation Youtube video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/achalhoub/miniconda3/envs/transformer_pt/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Specify device to use\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "embed_len = 512\n",
    "batch_size = 8\n",
    "stack_len = 6\n",
    "dropout = 0.1\n",
    "\n",
    "output_vocab_size = 7000    # just for testing\n",
    "input_vocab_size = 7000     # just for testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the embedding block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, input_vocab_size=input_vocab_size, embed_len=embed_len, dropout=dropout, device=device):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.embed_len = embed_len\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "\n",
    "        self.firstEmbedding = nn.Embedding(self.input_vocab_size, self.embed_len)       # first embedding layer\n",
    "        self.secondEmbedding = nn.Embedding(self.input_vocab_size, self.embed_len)      # positional embedding layer\n",
    "\n",
    "        self.dropoutLayer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "    # input would be of size (batch_size, seq_len)\n",
    "    def forward(self, input):\n",
    "        first_embedding = self.firstEmbedding(input)\n",
    "        batch_size, seq_len = input.shape\n",
    "\n",
    "        # positions_vector should also be of dimension (batch_size, seq_len)\n",
    "        positions_vector = torch.arange(0, seq_len).expand(batch_size, seq_len).to(self.device)\n",
    "        positional_encoding = self.secondEmbedding(positions_vector)\n",
    "\n",
    "        # output has shape (batch_size, seq__len, embed_len)\n",
    "        return self.dropoutLayer(first_embedding + positional_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "input_test = torch.randint(10, (8, 20)).to(device)\n",
    "embedding_layer = InputEmbedding().to(device)\n",
    "output = embedding_layer.forward(input_test)\n",
    "print(output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Scaled Dot Product block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProduct(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len, mask=None):\n",
    "        super(ScaledDotProduct, self).__init__()\n",
    "        self.embed_len = embed_len\n",
    "        self.mask = mask\n",
    "        self.dk = embed_len         # dimension of keys and queries\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=3)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        compatibility = torch.matmul(queries, torch.transpose(keys, 2, 3))\n",
    "        compatibility = compatibility / math.sqrt(self.dk)\n",
    "\n",
    "        compatibility = self.softmax(compatibility)\n",
    "\n",
    "        # apply a mask for the decoder\n",
    "        if self.mask is not None:\n",
    "            compatibility = torch.tril(compatibility)\n",
    "\n",
    "        return torch.matmul(compatibility, torch.transpose(values, 1, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Multi Headed Implementation block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads=num_heads, embed_len=embed_len, batch_size=batch_size, mask=None):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_len = embed_len\n",
    "        self.batch_size = batch_size\n",
    "        self.mask = mask\n",
    "        self.head_length = int(self.embed_len/self.num_heads)\n",
    "\n",
    "        self.q_in = self.v_in = self.k_in = self.embed_len\n",
    "\n",
    "        # Linear layers as input to multiheaded attention.\n",
    "        # They have input dim = 512 and output dim = 512\n",
    "        self.q_linear = nn.Linear(int(self.q_in), int(self.q_in))\n",
    "        self.k_linear = nn.Linear(int(self.k_in), int(self.k_in))\n",
    "        self.v_linear = nn.Linear(int(self.v_in), int(self.v_in))\n",
    "\n",
    "        # activate the mask for decoder\n",
    "        if self.mask is not None:\n",
    "            self.attention = ScaledDotProduct(mask=True)\n",
    "        else:\n",
    "            self.attention = ScaledDotProduct()\n",
    "        \n",
    "        # final output linear layer\n",
    "        self.output_linear = nn.Linear(self.q_in, self.q_in)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "\n",
    "        # queries shape = (8, 20, 512).\n",
    "        # we need to reshape (batch_size, seq_len, embed_len) to (batch_size, num_heads, seq_len, head_length)\n",
    "        # output should be reshaped into (8, 8, 20, 64)\n",
    "        queries = self.q_linear(queries).reshape(\n",
    "            self.batch_size, -1, self.num_heads, self.head_length\n",
    "        )\n",
    "        queries = queries.transpose(1, 2)\n",
    "\n",
    "        # keys shape = (batch_size, num_heads, seq_len, head_length)\n",
    "        keys = self.k_linear(keys).reshape(\n",
    "            self.batch_size, -1, self.num_heads, self.head_length\n",
    "        )\n",
    "        keys = keys.transpose(1, 2)\n",
    "\n",
    "        # values shape = (batch_size, seq_len, num_heads, head_length)\n",
    "        values = self.v_linear(values).reshape(\n",
    "            self.batch_size, -1, self.num_heads, self.head_length\n",
    "        )\n",
    "\n",
    "        # QK result dimension = (batch_size, num_heads, seq_len, seq_len) -> (8, 8, 20, 20)\n",
    "        # QK matmul V -> (8, 8, 20, 20) matmul (8, 8, 20, 64) = (8, 8, 20, 64).\n",
    "        # final shape should be (batch_size, seq_len, embed_len)\n",
    "        sdp_output = self.attention.forward(queries, keys, values).transpose(1, 2).reshape(\n",
    "            self.batch_size, -1, self.num_heads*self.head_length   \n",
    "        )\n",
    "\n",
    "        # output has size (8, 20, 512)\n",
    "        return self.output_linear(sdp_output)        \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the encoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len, dropout=dropout):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.embed_len = embed_len\n",
    "        self.dropout = dropout\n",
    "        self.multihead = MultiHeadAttention()\n",
    "        self.firstNorm = nn.LayerNorm(self.embed_len)\n",
    "        self.secondNorm = nn.LayerNorm(self.embed_len)\n",
    "        self.dropoutLayer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        self.feedForward = nn.Sequential(\n",
    "            nn.Linear(self.embed_len, self.embed_len*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embed_len*4, self.embed_len)\n",
    "        )\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        attention_output = self.multihead.forward(queries, keys, values)\n",
    "        attention_output = self.dropoutLayer(attention_output)\n",
    "        first_sublayer_output = self.firstNorm(attention_output + queries)\n",
    "\n",
    "        ff_output = self.feedForward(first_sublayer_output)\n",
    "        ff_output = self.dropoutLayer(ff_output)\n",
    "        \n",
    "        return self.secondNorm(ff_output + first_sublayer_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the decoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len, dropout=dropout):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.embed_len = embed_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.maskedMultihead = MultiHeadAttention(mask=True)\n",
    "        self.firstNorm = nn.LayerNorm(self.embed_len)\n",
    "        self.dropoutLayer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        self.encoderBlock = EncoderBlock()\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        masked_multihead_output = self.maskedMultihead.forward(queries, queries, queries)\n",
    "        masked_multihead_output = self.dropoutLayer(masked_multihead_output)\n",
    "        first_sublayer_output = self.firstNorm(masked_multihead_output + queries)\n",
    "\n",
    "        return self.encoderBlock(first_sublayer_output, keys, values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement full transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len, stack_len=stack_len, device=device, output_vocab_size=output_vocab_size):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embed_len = embed_len\n",
    "        self.stack_len = stack_len\n",
    "        self.device = device\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "\n",
    "        self.embedding = InputEmbedding().to(self.device)\n",
    "        self.encStack = nn.ModuleList(EncoderBlock() for i in range(self.stack_len)).to(self.device)\n",
    "        self.decStack = nn.ModuleList(DecoderBlock() for i in range(self.stack_len)).to(self.device)\n",
    "\n",
    "        self.finalLinear = nn.Linear(self.embed_len, self.output_vocab_size).to(self.device)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, test_input, test_output):\n",
    "        enc_output = self.embedding.forward(test_input)\n",
    "\n",
    "        for enc_layer in self.encStack:\n",
    "            enc_output = enc_layer.forward(enc_output, enc_output, enc_output)\n",
    "\n",
    "        dec_output = self.embedding(test_output)\n",
    "        for dec_layer in self.decStack:\n",
    "            dec_output = dec_layer.forward(dec_output, enc_output, enc_output)\n",
    "        \n",
    "        final_output = self.finalLinear(dec_output)\n",
    "\n",
    "        return self.softmax(final_output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18591/2299604942.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.softmax(final_output)\n"
     ]
    }
   ],
   "source": [
    "input_tokens = torch.randint(10, (batch_size, 30)).to(device)\n",
    "output_target = torch.randint(10, (batch_size, 20)).to(device)\n",
    "\n",
    "transformer = Transformer().to(device)\n",
    "transformer_output = transformer.forward(input_tokens, output_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 20, 7000])\n"
     ]
    }
   ],
   "source": [
    "print(transformer_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe19fb5bed516e6ac74a9b255f69d3cfb16eeb4b5a81b7a48fdeb18da73b6471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
